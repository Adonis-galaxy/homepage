<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="icon" type="image/png" href="images/Yale.png">
  <title>Ziyao Zeng</title>

  <!-- Google Fonts -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet" />

  <!-- Custom styles -->
  <link href="css/style.css" rel="stylesheet">
</head>
<body>
  <!-- Navigation -->
  <nav class="main-nav">
    <div class="nav-container">
      <div class="nav-brand">Ziyao Zeng</div>
      <ul class="nav-links">
        <li><a href="#home" class="nav-link active">Home</a></li>
        <li><a href="#research" class="nav-link">Research</a></li>
        <li><a href="#others" class="nav-link">Others</a></li>
      </ul>
    </div>
  </nav>

  <!-- Hero Section -->
  <section id="home" class="hero-section">
    <div class="container">
      <div class="hero-content">
        <div class="hero-text">
          <h1 class="hero-title">
            <span class="name-main">Ziyao (Adonis) Zeng</span>
            <!-- <span class="name-accent"></span> -->
            <span class="name-chinese">曾子尧</span>
          </h1>

          <p class="hero-subtitle">
            Ph.D. Student in Computer Science<br>
            <span class="institution">Yale University</span>
          </p>

          <div class="hero-description">
            <p>I'm a third-year Ph.D. student in Computer Science (2023 - [Expected] 2027) at <a href="https://www.yale.edu/">Yale University</a>. Previous to that, I obtained my B.Eng. in Computer Science (2019 - 2023) at <a href="https://www.shanghaitech.edu.cn/eng/">ShanghaiTech University</a>, minor in <a href="https://sem.shanghaitech.edu.cn/sem_en/2017/0724/c3725a29609/page.htm">Innovation and Entrepreneurship</a>.</p>

            <p>I conduct research on <strong>Multimodal Learning</strong> inspired by human cognition towards <strong>Multimodal Agentic AI</strong>, especially <strong>vision-language models</strong> and <strong>spatial understanding</strong>. My line of work in <em>"Language for 3D Vision"</em> explores <u>how vision-language models can perceive and understand the world like humans do</u>. My expertise lies in vision-language models, large-language models, diffusion models, multimodal learning, and 2D/3D computer vision.</p>
          </div>

          <div class="hero-links">
            <a href="https://scholar.google.com/citations?hl=en&user=FYL2DYEAAAAJ" class="link-btn">
              <svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm-2 15l-5-5 1.41-1.41L10 14.17l7.59-7.59L19 8l-9 9z"/>
              </svg>
              Google Scholar
            </a>
            <a href="http://www.linkedin.com/in/ziyao-zeng-52372a29b" class="link-btn">
              <svg width="16" height="16" viewBox="0 0 24 24" fill="currentColor">
                <path d="M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853 0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601 0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433c-1.144 0-2.063-.926-2.063-2.065 0-1.138.92-2.063 2.063-2.063 1.14 0 2.064.925 2.064 2.063 0 1.139-.925 2.065-2.064 2.065zm1.782 13.019H3.555V9h3.564v11.452zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2 0 22.222 0h.003z"/>
              </svg>
              LinkedIn
            </a>
            <a href="https://github.com/Adonis-galaxy" class="link-btn">
              <svg width="16" height="16" viewBox="0 0 24 24" fill="currentColor">
                <path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"/>
              </svg>
              GitHub
            </a>
            <a href="mailto:ziyao.zeng@yale.edu" class="link-btn">
              <svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                <path d="M4 4h16c1.1 0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1 0-2-.9-2-2V6c0-1.1.9-2 2-2z"/>
                <polyline points="22,6 12,13 2,6"/>
              </svg>
              Email
            </a>
          </div>

          <div class="collaborators-section">
            <h3>Collaborators</h3>
            <div class="collaborators-grid">
              <div class="collab-group">
                <strong>Nvidia Research</strong>
                <div class="collab-links">
                  <a href="https://oraziogallo.github.io/">Dr. Orazio Gallo</a>,
                  <a href="https://suhangpro.github.io/">Dr. Hang Su</a>,
                  <a href="https://www.jindongjiang.me/">Dr. Jindong Jiang</a>,
                  <a href="https://research.nvidia.com/person/abhishek-badki">Dr. Abhishek Badki</a>,
                  <a href="https://sifeiliu.net/">Dr. Sifei Liu</a>
                </div>
              </div>
              <div class="collab-group">
                <strong>Yale Vision Lab</strong>
                <div class="collab-links">
                  <a href="https://www.cs.yale.edu/homes/wong-alex/">Prof. Alex Wong</a>,
                  <a href="https://donglao.github.io/">Prof. Dong Lao</a>,
                  <a href="https://www.image.cau.ac.kr/people/director">Prof. Byung-Woo Hong</a>,
                  <a href="https://web.cs.ucla.edu/~soatto/">Prof. Stefano Soatto</a>
                </div>
              </div>
              <div class="collab-group">
                <strong>UPenn GRASP Lab</strong>
                <div class="collab-links">
                  <a href="https://www.cis.upenn.edu/~jshi/">Prof. Jianbo Shi</a>,
                  <a href="https://zrrskywalker.github.io/">Dr. Renrui Zhang</a>
                </div>
              </div>
              <div class="collab-group">
                <strong>ShanghaiTech PLUS Group</strong>
                <div class="collab-links">
                  <a href="https://xmhe.bitbucket.io/">Prof. Xuming He</a>,
                  <a href="https://gaozhitong.github.io/">Zhitong Gao</a>
                </div>
              </div>
            </div>
            <p class="collab-note">I was also fortunate to intern at <a href="https://www.shlab.org.cn/">Shanghai AI Lab</a> and <a href="https://www.uisee.com/en/">UISEE</a> during my undergraduate studies.</p>
          </div>

          <div class="reviewer-section">
            <h3>Reviewer Services</h3>
            <p>CVPR (2022, 2025 <strong>[Outstanding Reviewer]</strong>, 2026), ICCV (2023, 2025), ECCV (2024), ICML (2025), ICLR (2025, 2026), NeurIPS (2024, 2025), ACM MM (2023, 2025), AISTATS (2024, 2025), ICASSP (2024, 2025), TCSVT (journal)</p>
          </div>
        </div>

        <div class="hero-image">
          <img src="images/cover_full_body.jpg" alt="Ziyao Zeng" class="profile-img">
        </div>
      </div>
    </div>
  </section>

  <!-- Research Section -->
  <section id="research" class="research-section">
    <div class="container">
      <div class="section-header">
        <h2>Research</h2>
        <p class="section-subtitle">Multimodal Learning towards Multimodal Agentic AI</p>
      </div>

      <div class="research-overview">
        <p>Since the age of 13, deeply touched by <em><a href="https://en.wikipedia.org/wiki/Foundation_(Asimov_novel)">Foundation</a></em> by <a href="https://en.wikipedia.org/wiki/Isaac_Asimov">Isaac Asimov</a>, my dream has been to create an AI who can think like humans (just like the dream of <a href="https://people.idsia.ch//~juergen/">Prof. Jürgen Schmidhuber</a>). When humans perceive the surrounding environment, we see (2D vision), hear (audio), feel (tactile), and interact (3D vision) simultaneously to reason (language) and understand (neural signal) the world. Therefore, I conduct research on <strong>Multimodal Learning</strong> towards <strong>Multimodal Agentic AI</strong>. My research vision is to empower agentic AI with multimodal sensing and multimodal representations, enabling it to perceive, reason, understand, and interact with both the digital and physical worlds like humans do.</p>
        <p>Specifically, my line of work in <em>"Language for 3D Vision"</em> (DepthCLIP, PointCLIPv2, WorDepth, RSA, Iris) explores <u>how vision-language models can perceive and understand the world like humans do</u>.</p>
      </div>

      <div class="publications-header">
        <h3>Publications</h3>
        <p class="highlight-notice">Selected publications are highlighted.</p>
        <p class="contribution-note">(* indicates equal contributions)</p>
      </div>

      <!-- 2026 -->
      <div class="year-section">
        <h4 class="year-title">2026</h4>

        <div class="publication-card highlight">
          <div class="pub-image">
            <img src="research/icon/RuleSmith.png" alt="RuleSmith">
          </div>
          <div class="pub-content">
            <h5 class="pub-title"><a href="https://adonis-galaxy.github.io/RuleSmith-website/">RuleSmith: Multi-Agent LLMs for Automated Game Balancing</a></h5>
            <p class="pub-authors"><strong>Ziyao Zeng</strong>, Chen Liu, Tianyu Liu, Hao Wang, Xiatao Sun, Fengyu Yang, Xiaofeng Liu, Zhiwen Fan</p>
            <p class="pub-venue">2026 | <a href="https://adonis-galaxy.github.io/RuleSmith-website/">project page</a>, <a href="https://github.com/Adonis-galaxy/RuleSmith">code</a>, <a href="https://arxiv.org/abs/2602.06232">paper</a></p>
          </div>
        </div>
      </div>

      <!-- 2025 -->
      <div class="year-section">
        <h4 class="year-title">2025</h4>

        <div class="publication-card highlight">
          <div class="pub-image">
            <img src="research/icon/Coffee.png" alt="Coffee">
          </div>
          <div class="pub-content">
            <h5 class="pub-title"><a href="https://arxiv.org/abs/2511.14113">Coffee: Controllable Diffusion Fine-tuning</a></h5>
            <p class="pub-authors"><strong>Ziyao Zeng</strong>, Jingcheng Ni, Ruyi Liu, Alex Wong</p>
            <p class="pub-venue">arXiv technical report, 2025</p>
          </div>
        </div>

        <div class="publication-card">
          <div class="pub-image">
            <img src="research/icon/ETA.png" alt="ETA">
          </div>
          <div class="pub-content">
            <h5 class="pub-title"><a href="https://arxiv.org/abs/2508.05989">ETA: Energy-based Test-time Adaptation for Depth Completion</a></h5>
            <p class="pub-authors">Younjoon Chung*, Hyoungseob Park*, Patrick Rim*, Xiaoran Zhang, Jihe He, <strong>Ziyao Zeng</strong>, Safa Cicek, Byung-Woo Hong, James S. Duncan, Alex Wong</p>
            <p class="pub-venue">ICCV 2025</p>
          </div>
        </div>

        <div class="publication-card">
          <div class="pub-image">
            <img src="research/icon/ProtoDepth.png" alt="ProtoDepth">
          </div>
          <div class="pub-content">
            <h5 class="pub-title"><a href="https://arxiv.org/abs/2503.12745">ProtoDepth: Unsupervised Continual Depth Completion with Prototypes</a></h5>
            <p class="pub-authors">Patrick Rim, Hyoungseob Park, S. Gangopadhyay, <strong>Ziyao Zeng</strong>, Younjoon Chung, Alex Wong</p>
            <p class="pub-venue">CVPR 2025 | <a href="https://protodepth.github.io/">project page</a>, <a href="https://github.com/patrickqrim/ProtoDepth">code</a></p>
          </div>
        </div>

        <div class="publication-card">
          <div class="pub-image">
            <img src="research/icon/HOMER.png" alt="HOMER">
          </div>
          <div class="pub-content">
            <h5 class="pub-title"><a href="https://arxiv.org/abs/2501.17636">HOMER: Homography-Based Efficient Multi-view 3D Object Removal</a></h5>
            <p class="pub-authors">Jingcheng Ni*, Weiguang Zhao*, Daniel Wang, <strong>Ziyao Zeng</strong>, Chenyu You, Alex Wong, Kaizhu Huang</p>
            <p class="pub-venue">arXiv technical report, 2025</p>
          </div>
        </div>
      </div>

      <!-- 2024 -->
      <div class="year-section">
        <h4 class="year-title">2024</h4>

        <div class="publication-card highlight">
          <div class="pub-image">
            <img src="research/icon/Iris.png" alt="Iris">
          </div>
          <div class="pub-content">
            <h5 class="pub-title"><a href="https://adonis-galaxy.github.io/Iris-website/">Iris: Integrating Language into Diffusion-based Monocular Depth Estimation</a></h5>
            <p class="pub-authors"><strong>Ziyao Zeng*</strong>, Jingcheng Ni*, Daniel Wang, Patrick Rim, Younjoon Chung, Fengyu Yang, Byung-Woo Hong, Alex Wong</p>
            <p class="pub-venue">arXiv technical report, 2024 | <a href="https://necv2025.github.io/">NECV 2025 Oral Presentation (18.75%)</a> | <a href="https://adonis-galaxy.github.io/Iris-website/">project page</a>, <a href="https://github.com/Adonis-galaxy/Iris">code</a>, <a href="https://www.arxiv.org/abs/2411.16750">paper</a></p>
          </div>
        </div>

        <div class="publication-card highlight">
          <div class="pub-image">
            <img src="research/icon/RSA.png" alt="RSA">
          </div>
          <div class="pub-content">
            <h5 class="pub-title"><a href="http://arxiv.org/abs/2410.02924">RSA: Resolving Scale Ambiguities in Monocular Depth Estimators through Language Descriptions</a></h5>
            <p class="pub-authors"><strong>Ziyao Zeng</strong>, Yangchao Wu, Hyoungseob Park, Daniel Wang, Fengyu Yang, Stefano Soatto, Dong Lao, Byung-Woo Hong, Alex Wong</p>
            <p class="pub-venue">NeurIPS 2024 | <a href="https://github.com/Adonis-galaxy/RSA">code</a></p>
          </div>
        </div>

        <div class="publication-card">
          <div class="pub-image">
            <img src="research/icon/NeuroBind.png" alt="NeuroBind">
          </div>
          <div class="pub-content">
            <h5 class="pub-title"><a href="https://arxiv.org/abs/2407.14020">NeuroBind: Towards Unified Multimodal Representations for Neural Signals</a></h5>
            <p class="pub-authors">Fengyu Yang*, Chao Feng*, Daniel Wang*, Tianye Wang, <strong>Ziyao Zeng</strong>, Zhiyang Xu, Hyoungseob Park, Pengliang Ji, Hanbin Zhao, Yuanning Li, Alex Wong</p>
            <p class="pub-venue">arXiv technical report, 2024</p>
          </div>
        </div>
      </div>

      <!-- 2023 -->
      <div class="year-section">
        <h4 class="year-title">2023</h4>

        <div class="publication-card highlight">
          <div class="pub-image">
            <img src="research/icon/WorDepth.png" alt="WorDepth">
          </div>
          <div class="pub-content">
            <h5 class="pub-title"><a href="https://arxiv.org/abs/2404.03635">WorDepth: Variational Language Prior for Monocular Depth Estimation</a></h5>
            <p class="pub-authors"><strong>Ziyao Zeng</strong>, Daniel Wang, Fengyu Yang, Hyoungseob Park, Yangchao Wu, Stefano Soatto, Byung-Woo Hong, Dong Lao, Alex Wong</p>
            <p class="pub-venue">CVPR 2024 | <a href="https://github.com/Adonis-galaxy/WorDepth">code</a></p>
          </div>
        </div>

        <div class="publication-card">
          <div class="pub-image">
            <img src="research/icon/UniTouch.png" alt="UniTouch">
          </div>
          <div class="pub-content">
            <h5 class="pub-title"><a href="https://arxiv.org/abs/2401.18084">Binding Touch to Everything: Learning Unified Multimodal Tactile Representations</a></h5>
            <p class="pub-authors">Fengyu Yang*, Chao Feng*, Ziyang Chen*, Hyoungseob Park, Daniel Wang, Yiming Dou, <strong>Ziyao Zeng</strong>, Xien Chen, Rit Gangopadhyay, Andrew Owens, Alex Wong</p>
            <p class="pub-venue">CVPR 2024 | <a href="https://cfeng16.github.io/UniTouch/">project page</a>, <a href="https://github.com/cfeng16/UniTouch">code</a></p>
          </div>
        </div>
      </div>

      <!-- 2022 -->
      <div class="year-section">
        <h4 class="year-title">2022</h4>

        <div class="publication-card highlight">
          <div class="pub-image">
            <img src="research/icon/PointCLIPv2.png" alt="PointCLIPv2">
          </div>
          <div class="pub-content">
            <h5 class="pub-title"><a href="https://arxiv.org/abs/2211.11682">PointCLIP V2: Prompting CLIP and GPT for Powerful 3D Open-world Learning</a></h5>
            <p class="pub-authors">Xiangyang Zhu*, Renrui Zhang*, Bowei He, Ziyu Guo, <strong>Ziyao Zeng</strong>, Zipeng Qin, Shanghang Zhang, Peng Gao</p>
            <p class="pub-venue">ICCV 2023 | <a href="https://github.com/yangyangyang127/PointCLIP_V2">code</a></p>
          </div>
        </div>

        <div class="publication-card">
          <div class="pub-image">
            <img src="research/icon/iQuery.png" alt="iQuery">
          </div>
          <div class="pub-content">
            <h5 class="pub-title"><a href="https://arxiv.org/abs/2212.03814">iQuery: Instruments as Queries for Audio-Visual Sound Separation</a></h5>
            <p class="pub-authors">Jiaben Chen, Renrui Zhang, Dongze Lian, Jiaqi Yang, <strong>Ziyao Zeng</strong>, Jianbo Shi</p>
            <p class="pub-venue">CVPR 2023 | <a href="https://github.com/JiabenChen/iQuery">code</a></p>
          </div>
        </div>

        <div class="publication-card highlight">
          <div class="pub-image">
            <img src="research/icon/depthclip.png" alt="DepthCLIP">
          </div>
          <div class="pub-content">
            <h5 class="pub-title"><a href="https://arxiv.org/abs/2207.01077">Can Language Understand Depth?</a></h5>
            <p class="pub-authors">Renrui Zhang*, <strong>Ziyao Zeng*</strong>, Ziyu Guo, Yafeng Li</p>
            <p class="pub-venue">ACM Multimedia 2022, accepted as Brave New Idea (Accepte Rate<=12.5%) | <a href="https://github.com/Adonis-galaxy/DepthCLIP">code</a></p>
          </div>
        </div>
      </div>

      <!-- 2021 -->
      <div class="year-section">
        <h4 class="year-title">2021</h4>

        <div class="publication-card">
          <div class="pub-image">
            <img src="research/icon/dspoint.png" alt="DSPoint">
          </div>
          <div class="pub-content">
            <h5 class="pub-title"><a href="https://arxiv.org/abs/2111.10332">DSPoint: Dual-scale Point Cloud Recognition with High-frequency Fusion</a></h5>
            <p class="pub-authors">Renrui Zhang*, <strong>Ziyao Zeng*</strong>, Ziyu Guo, Xinben Gao, Kexue Fu, Jianbo Shi</p>
            <p class="pub-venue">SMC 2023 | <a href="https://github.com/Adonis-galaxy/DSPoint">code</a></p>
          </div>
        </div>

        <div class="publication-card">
          <div class="pub-image">
            <img src="research/icon/vtclip.png" alt="VT-CLIP">
          </div>
          <div class="pub-content">
            <h5 class="pub-title"><a href="https://arxiv.org/abs/2112.02399">VT-CLIP: Enhancing Vision-Language Models with Visual-guided Texts</a></h5>
            <p class="pub-authors">Longtian Qiu, Renrui Zhang, Ziyu Guo, <strong>Ziyao Zeng</strong>, Yafeng Li, Guangnan Zhang</p>
            <p class="pub-venue">arXiv technical report, 2021</p>
          </div>
        </div>

        <div class="publication-card">
          <div class="pub-image">
            <img src="research/icon/AIFinal.png" alt="Twitter Emotion">
          </div>
          <div class="pub-content">
            <h5 class="pub-title"><a href="research/AI_CS181_project.pdf">Twitter Emotion Classification</a></h5>
            <p class="pub-authors">Yiteng Xu*, <strong>Ziyao Zeng*</strong>, Jirui Shi*, Shaoxun Wu*, Peiyan Gu*</p>
            <p class="pub-venue">Final Project of CS181 Artificial Intelligence, 2021 Fall, ShanghaiTech University | <a href="https://github.com/Adonis-galaxy/Twitter-Emotion-Classification">code</a></p>
          </div>
        </div>

        <div class="publication-card">
          <div class="pub-image">
            <img src="research/icon/MLFinal.png" alt="Generalized DUQ">
          </div>
          <div class="pub-content">
            <h5 class="pub-title"><a href="research/ML_CS282_project.pdf">Generalized DUQ: Generalized Deterministic Uncertainty Quantification</a></h5>
            <p class="pub-authors">Zhitong Gao*, <strong>Ziyao Zeng*</strong></p>
            <p class="pub-venue">Final Project of CS282 Machine Learning, 2021 Spring, ShanghaiTech University</p>
          </div>
        </div>
      </div>

      <!-- 2020 -->
      <div class="year-section">
        <h4 class="year-title">2020</h4>

        <div class="publication-card">
          <div class="pub-image">
            <img src="research/icon/DLFinal.png" alt="Noisy Labels">
          </div>
          <div class="pub-content">
            <h5 class="pub-title"><a href="research/DL_CS280_project.pdf">Seek Common while Shelving Differences: A New Way for dealing with Noisy Labels</a></h5>
            <p class="pub-authors">Zhitong Gao*, <strong>Ziyao Zeng*</strong></p>
            <p class="pub-venue">Final Project of CS280 Deep Learning, 2020 Fall, ShanghaiTech University</p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Others Section -->
  <section id="others" class="others-section">
    <div class="container">
      <div class="section-header">
        <h2>Others</h2>
      </div>

      <div class="others-content">
        <div class="others-item">
          <h3>Unity Game Development</h3>
          <p>I'm an amateur Unity game developer, previous supervised by <a href="http://www.briancox.be/?page=home">Brain Cox</a>, screenshots of my previous works have been shown below.</p>

          <div class="game-showcase">
            <div class="game-item">
              <h4>Snow Ranger</h4>
              <img src="images/Unity/Unity_HW2.png" alt="Snow Ranger" class="game-img">
            </div>
            <div class="game-item">
              <h4><a href="https://www.bilibili.com/video/BV1WS4y1Z7rz?p=1&share_medium=ipad&share_plat=ios&share_source=QQ&share_tag=s_i&timestamp=1641184133&unique_k=GHTLVUs">Darkside</a></h4>
              <div class="game-screenshots">
                <img src="images/Unity/main_menu.jpg" alt="Darkside Main Menu" class="game-img-small">
                <img src="images/Unity/night_vision.jpg" alt="Darkside Night Vision" class="game-img-small">
                <img src="images/Unity/puzzle.jpg" alt="Darkside Puzzle" class="game-img-small">
              </div>
            </div>
          </div>
        </div>

        <div class="others-item">
          <h3>Music & Arts</h3>
          <p>I'm also an amateur pianist, trombone player, guitar player, and Chinese folk singer.</p>
          <p>I have been playing Tarot since 2014, dedicating to combining Tarot with modern psychology to serve as a tool for consciousness.</p>
        </div>

        <div class="others-item">
          <h3>Volunteering</h3>
          <p>Previously, I volunteered at <a href="http://www.wwfchina.org/">WWF-China</a> and <a href="http://www.greenpeace.org.cn/">Greenpeace</a></p>
        </div>
      </div>
    </div>
  </section>

  <!-- Footer -->
  <footer class="main-footer">
    <div class="container">
      <div class="footer-content">
        <!-- <p class="footer-note">Website format from <a href="https://www.cs.utexas.edu/~zhouxy/#publication">Xingyi Zhou</a>.</p> -->
        <p class="footer-update">Last updated Feb. 2026</p>
      </div>
    </div>
  </footer>

  <!-- Visitor Counter - light background so widget is visible -->
  <div class="visitor-counter-wrap">
    <div class="container">
      <p class="visitor-counter-label">Page views</p>
      <div class="visitor-counter">
        <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=080808&w=300&t=tt&d=yuoBMXDrKL-QBzjeq858G6uTHIVF7jwRwEfLziX7UYU&co=ffffff&cmo=3acc3a&cmn=ff5353&ct=808080'></script>
      </div>
    </div>
  </div>

  <script>
    // Smooth scroll navigation
    document.querySelectorAll('.nav-link').forEach(link => {
      link.addEventListener('click', function(e) {
        e.preventDefault();
        const targetId = this.getAttribute('href').substring(1);
        const targetSection = document.getElementById(targetId);

        if (targetSection) {
          const offset = 80;
          const targetPosition = targetSection.offsetTop - offset;

          window.scrollTo({
            top: targetPosition,
            behavior: 'smooth'
          });

          // Update active nav link
          document.querySelectorAll('.nav-link').forEach(l => l.classList.remove('active'));
          this.classList.add('active');
        }
      });
    });

    // Update active nav on scroll
    window.addEventListener('scroll', function() {
      const sections = ['home', 'research', 'others'];
      const scrollPosition = window.pageYOffset + 150;

      sections.forEach(sectionId => {
        const section = document.getElementById(sectionId);
        if (section) {
          const sectionTop = section.offsetTop;
          const sectionHeight = section.offsetHeight;

          if (scrollPosition >= sectionTop && scrollPosition < sectionTop + sectionHeight) {
            document.querySelectorAll('.nav-link').forEach(link => {
              link.classList.remove('active');
              if (link.getAttribute('href') === '#' + sectionId) {
                link.classList.add('active');
              }
            });
          }
        }
      });
    });
  </script>
</body>
</html>
