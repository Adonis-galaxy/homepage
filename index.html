
<!DOCTYPE html>
<html lang="en">
  
  <head>
    <meta charset="utf-8">
    <link rel="icon" type="image/png" href="images/3838_icon.png">
  	<meta charset="utf-8">

    <style type="text/css">
      a:link {
        color:rgb(0, 47, 164);
        text-decoration:none;
      }
      a:visited {
        color:rgb(0, 47, 164);
        text-decoration:none;
      }
      a:hover {
        color:rgb(0, 47, 164);
        text-decoration:none;
      }
      a:active {
        color:rgb(0, 47, 164);
        text-decoration:none;
      }
      
      
    </style>
    <title>Ziyao Zeng</title>

    <!-- Bootstrap core CSS -->
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">

    <!-- Custom styles for this template -->
    <link href="css/style.css" rel="stylesheet">
  </head>

  <body>

    <div class="container col-xs-12 col-sm-8 col-md-8 col-lg-8 col-md-offset-2 col-lg-offset-2 col-sm-offset-2 p-0">

        <h1 id = "myName" align ="center">Ziyao
            <span class="text-primary">Zeng</span> (Adonis)
        </h1>
<!-- 按钮设置 -->
        <ul class="nav nav-tabs" id="myTab" role="tablist">
        <li class="nav-item">
          <a class="nav-link active" id="home-tab" data-toggle="tab" href="#home" role="tab" aria-controls="home" aria-selected="true">Home</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" id="publication-tab" data-toggle="tab" href="#publication" role="tab" aria-controls="publication" aria-selected="false">Research</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" id="enterprise-tab" data-toggle="tab" href="#enterprise" role="tab" aria-controls="enterprise" aria-selected="false">Enterprise</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" id="adventure-tab" data-toggle="tab" href="#adventure" role="tab" aria-controls="adventure" aria-selected="false">Adventure</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" id="others-tab" data-toggle="tab" href="#others" role="tab" aria-controls="others" aria-selected="false">Others</a>
        </li>
      </ul>
      <div class="tab-content" id="nav-tabContent">
<!-- 正文 -->
  <!-- Introduction -->
    <div class="tab-pane fade show active" id="home" role="tabpanel" aria-labelledby="home-tab">

     
        <table>
          <tr>
            <td width = "62%">
              <p align>    
                <!-- <h4 style="margin-top: 10px">Starting AI-Scientist with Enterprise</h4>    -->

                I'm a fourth year (2019 - [Expected] 2023) Computer Science undergraduate student at <a href="https://www.shanghaitech.edu.cn/eng/">ShanghaiTech University</a>.
          
                I am currently interning with <a href="https://www.cis.upenn.edu/~jshi/">Prof. Jianbo Shi</a> at <a href="https://www.grasp.upenn.edu/">UPenn GRASP Lab</a>. 
                Previously, I interned with <a href="https://xmhe.bitbucket.io/">Prof. Xuming He</a> at <a href="http://plus.sist.shanghaitech.edu.cn/">ShanghaiTech PLUS Group</a>.    
                I have served as a reviewer of <a href="https://cvpr2022.thecvf.com/">CVPR 2022</a>. 

                <!-- <li>A sci-fi novelist, Unity game developer, Tarot player, outdoor adventurer, body builder, and volunteer at <a href="http://www.wwfchina.org/">WWF-China</a> and <a href="http://www.greenpeace.org.cn/">Greenpeace</a>.</li> -->
                <p></p>
                I conduct research on <strong>Multimodality Perception</strong> that simulates human thinking. Currently, my research interest mainly covers <strong>3D Perception</strong> and <strong>Vision-Language Models</strong>.
                <p></p>
                As for publications so far: I have 1 ACMMM (Brave New Idea) paper, 2 arXiv papers, 3 open-source course projects.
                <p></p>
                <a href="Short CV-Ziyao Zeng.pdf">Short CV</a> /
                <!-- <a href="Long CV-Ziyao Zeng.pdf">Long CV</a> / -->
                <a href="https://scholar.google.com/citations?hl=en&user=FYL2DYEAAAAJ">Google Scholar</a> /
                <a href="https://www.linkedin.com/in/ziyaozeng/">LinkedIn</a> / 
                <a href="https://github.com/Adonis-galaxy">GitHub</a> /
                <a href="http://plus.sist.shanghaitech.edu.cn/author/ziyao-zeng/">PLUS Group</a>  
                <p></p>
                Email: zengzy@@shanghaitech.edu.cn
                
                
            </td>
            <td width = "1%">
            </td>
            <td width = "30%">
              <img src="images/cover_full_body.jpg" alt="PontTrust" align="middle" style="width: 100%;">
            </td>
        </tr>
        
        </table>
        <br>
        <p>I am looking for a Ph.D position starting at fall 2023.</p>
        <p id = "linkToJiarui">Website format from <a href="https://www.cs.utexas.edu/~zhouxy/#publication">Xingyi Zhou</a>.</p>
        <p align="center"> Last updated Sept. 2022</p>  <!-- Time -->
        </div>
        
        <!-- Research -->
        <div class="tab-pane fade" id="publication" role="tabpanel" aria-labelledby="publication-tab">
              <h4 style="margin-top: 10px">Research Overview</h4>
              <p>
                <li>Since the age of 13, deeply touched by <a href="https://en.wikipedia.org/wiki/Foundation_(Asimov_novel)">Foundation</a> written by <a href="https://en.wikipedia.org/wiki/Isaac_Asimov">Isaac Asimov</a>, my dream has been to colonize other planets, build the Galactic Empire and become its prime minister. This job can't be done by the current status of human beings, so my goal has turned to create an AI who can think like humans (just like the dream of <a href="https://people.idsia.ch//~juergen/">Prof. Jürgen Schmidhuber</a>) and conquer the galaxy together.</li>

                

                <!-- <li>Towards this goal, my research interest lies towards the strong AI in all kinds of topics related to perception, learning, and understanding in Computer Vision and Machine Learning. Includes but not limited to Point Cloud Learning, Monocular 3D Object Detection, and Uncertainty Estimation.</li>

                <li>I'm currently a research intern of <a href="https://www.grasp.upenn.edu/">GRASP Lab</a> of <a href="https://www.upenn.edu/">University of Pennsylvania</a> supervised by <a href="https://www.cis.upenn.edu/~jshi/">Prof. Jianbo Shi.</a></li>      

                <li>Besides, I'm also a research intern of <a href="http://plus.sist.shanghaitech.edu.cn/">PLUS Group</a> of <a href="https://www.shanghaitech.edu.cn/">ShanghaiTech University</a> supervised by <a href="https://xmhe.bitbucket.io/">Prof. Xuming He.</a></li>

                <li>I servered as a reviewer of CVPR 2022. -->
                  <p></p>
                <li>An AI who can think like humans must efficiently perceive surrounding information by combining features from different modalities. Towards this goal, I conduct research on <strong>Multimodality Perception</strong> that simulates human thinking. Currently, my research interest mainly covers <strong>3D Perception</strong> and <strong>Vision-Language Models</strong>.</li>
                  <p></p>
                  <!-- <p>1. <strong>Strong Perception:</strong> Point Cloud Learning, Monocular 3D Object Detection, Mix-data Anomaly Segmentation, 6D Pose Estimation using NeRF, Adversarial Training in NeRF......</p>
                  
                  <p>2. <strong>Explainable Model:</strong> Uncertainty Estimation, Learning with Noisy Labels, Reconstruction-based Self-supervised Image Segmentation, Audio-Visual Sound Separation, ...... </p>

                  <p>3. <strong>Cross-Modality: </strong> Vision-Lauguage Contrastive Learning Adapter, Point Cloud Joint Learning with Projection, Zero-shot Training-free Prompt-based Depth Estimation...... </p>

                  <p>Especially, I'm quite interested in how to make intelligence more explainable so that it can eventually be understood and defined. Therefore, I'm currently more focused on explainable models(in the area of strong perception and cross-modality).</I></p>
                  
                  <p>P.S. Projects after colon are those I have done before. -->

                <!-- <li>Besides that, I'm also conducting research on <strong>Reinforcement Learning</strong> and <strong>Generative Adversarial Networks</strong>, both of which are also keys to AGI.</li> -->

                <!-- <li>My current research plan is to create a prototype of human-interactable AGI by 2035, like <a href="https://bladerunner.fandom.com/wiki/Joi">Joi</a> in <a href="https://www.imdb.com/title/tt1856101/">Blade Runner 2049</a> or <a href="https://www.imdb.com/title/tt1798709/characters/nm0424060">Samantha</a> in <a href="https://www.imdb.com/title/tt1798709/?ref_=nv_sr_srsg_3">Her</a>, and create an AGI smarter than myself by 2060, like <a href="https://www.imdb.com/title/tt0133093/?ref_=fn_al_tt_1">the Matrix</a> or <a href="https://en.wikipedia.org/wiki/Skynet_(Terminator)">Skynet</a> in <a href="https://www.imdb.com/title/tt0088247/">the Terminator</a>. It's a long and highly risky journey, but I believe with the help of powerful AGI, our human race could achieve next level and set foot in the Galaxy.</li> -->
            
                <li>In recent years, this goal has become more and more realizable. CLIP jointly trains an image encoder and a text encoder and does contrastive learning in feature space. It stimulates the process that parents pointing to other objects to help children understand them. In this simple way, children could not only learn to classify objects but also to segment and detect them. A lot of work extends CLIP to complex cognitive tasks like segmentation and detection, which push AI to real intelligence a little bit further. For our DepthCLIP as an example, humans learn to predict depth not by pixel-wise depth annotation, but by relative depth information. A child will be taught this tree is far, and that bus is close. Finally, he or she would build a semantic depth understanding of seen monocular images, telling which part is near, and which part is remote. As for DepthCLIP, we found that CLIP (Contrastive Language-Image Pretraining) is also pre-trained with a pre-text task of semantic language-image concept understanding, and thus has the same ability to distinguish relative depth as humans. Then, we designed a projection scheme to obtain the quantified depth prediction. Surprisingly, our DepthCLIP surpasses existing unsupervised methods and even approaches the early fully-supervised networks, which proves that we could obtain efficient AI system by simulating human learning.</li>
            
                <!-- <li>My current research plan is to create a prototype of human-interactable AGI by 2035, like <a href="https://bladerunner.fandom.com/wiki/Joi">Joi</a> in <a href="https://www.imdb.com/title/tt1856101/">Blade Runner 2049</a>, and create an AGI smarter than myself by 2060, like <a href="https://en.wikipedia.org/wiki/Skynet_(Terminator)">the Skynet</a>. It's a long and highly risky journey, but I believe with the help of powerful AGI, our human race could achieve next level and set foot in the Galaxy.</li> -->
                <p></p>
                <!-- <li>For my detailed research experience, please see my <a href="Long CV-Ziyao Zeng.pdf">Long CV</a>.</li> -->
              </p>

              <!-- Publications -->
              <h4>Publications</h4>
              (* indicates equal contributions, † indicates the corresponding author)
              <h5>2022</h5>
              <hr>
              <div class="resume-item d-flex flex-column flex-md-row">
                <div class="resume-content mr-auto">
                  <img src="research/icon/depthclip.png" alt="PontTrust" align="middle" style="width: 70%;display:block;margin-left: auto;margin-right: auto;">
                  <br />
                  <strong><papertitle class=blue><a href="https://arxiv.org/abs/2207.01077">Can Language Understand Depth?</a></papertitle></strong>
                  <br>Renrui Zhang*, <strong>Ziyao Zeng*</strong>, Ziyu Guo, Yafeng Li<br>
                  <span>ACM Multimedia 2022 (Brave New Idea)</span><br>
                  <a href="https://github.com/Adonis-galaxy/DepthCLIP">code</a>
                  <font size="2.5"><li>We are the first to conduct zero-shot adaptation from the semantic language knowledge to quantified downstream vision tasks and perform zero-shot training-free monocular depth estimation. </li>
                  <li>Our DepthCLIP surpasses existing unsupervised methods and even approaches the early fully-supervised networks. </li></font>
                </div>
              </div>
              <br>

              <!-- <hr>
              <div class="resume-item d-flex flex-column flex-md-row">
                <div class="resume-content mr-auto">
                  <img src="research/icon/audio_sound.png" alt="PontTrust" align="middle" style="width: 70%;display:block;margin-left: auto;margin-right: auto;">
                  <br />
                  <strong><papertitle class=blue>Rethinking Audio-Visual Sound Separation with Mask Transformers</papertitle></strong>
                  <br>Jiaben Chen, Renrui Zhang, Dongze Lian, <strong>Ziyao Zeng</strong>, Jiaqi Yang, Xinhang Liu, Jianbo Shi<br>
                  <span>In submission</span><br>
                  <font size="2.5"><li>We re-formulate visual-sound separation as a sound Mask Transformer task, with a flexible query expansion mechanism. </li>
                  <li>We demonstrate state-of-the-art audio-visual sound source separation performance, with up to 44.2 % improvement of SDR on MUSIC benchmark.</li></font>
              </div>
              </div>
              <br> -->


              <h5>2021</h5>
                <hr>
                <div class="resume-item d-flex flex-column flex-md-row">
                  <div class="resume-content mr-auto">
                    <img src="research/icon/dspoint.png" alt="PontTrust" align="middle" style="width: 70%;display:block;margin-left: auto;margin-right: auto;">
                    <br />
                    <strong><papertitle class=blue><a href="https://arxiv.org/abs/2111.10332">DSPoint: Dual-scale Point Cloud Recognition with High-frequency Fusion</a></papertitle></strong>
                    <br>Renrui Zhang*, <strong>Ziyao Zeng*</strong>, Ziyu Guo, Xinben Gao, Kexue Fu, Jianbo ShXing† <br>
                    <span>arXiv technical report, 2021, in submission</span><br>
                    <a href="https://github.com/Adonis-galaxy/DSPoint">code</a>
                    <font size="2.5"><li>To deal with point cloud processing, we proposed DSPoint to conduct dual-scale processing: one by point-wise convolution for fine-grained geometry parsing, the other by voxel-wise global attention for long-range structural exploration.</li>
                    <li>We design a co-attention fusion module for feature alignment to blend local-global modalities, which conducts inter-scale cross-modality interaction by communicating high-frequency coordinates information.</li></font>
                  </div>
                </div>
                <br>

                
                <hr>
                <div class="resume-item d-flex flex-column flex-md-row">
                  <div class="resume-content mr-auto">
                    <img src="research/icon/vtclip.png" alt="PontTrust" align="middle" style="width: 70%;display:block;margin-left: auto;margin-right: auto;">
                    <br />
                    <strong><papertitle class=blue><a href="https://arxiv.org/abs/2112.02399">VT-CLIP: Enhancing Vision-Language Models with Visual-guided Texts</a></papertitle></strong>
                    <br>Renrui Zhang*, Longtian Qiu*, Wei Zhang, <strong>Ziyao Zeng</strong><br>
                    <span>arXiv technical report, 2021, in submission</span><br>
                    <font size="2.5"><li>To improve downstream adaptation of Contrastive Vision-Language Pre-training (CLIP), we propose VT-CLIP to enhance vision-language modeling via visual-guided texts. </li>
                    <li>Specifically, we guide the text feature to adaptively explore informative regions on the image and aggregate the visual feature by cross-attention.</li></font>
                  </div>
                </div>
                <br>


                <hr>
                <div class="resume-item d-flex flex-column flex-md-row">
                  <div class="resume-content mr-auto">
                    <img src="research/icon/AIFinal.png" alt="PontTrust" align="middle" style="width: 70%;display:block;margin-left: auto;margin-right: auto;">
                    <br />
                    <strong><papertitle class=blue><a href="research/AI_CS181_project.pdf">Twitter Emotion Classification</a></papertitle></strong>
                    <br>Yiteng Xu*, <strong>Ziyao Zeng*</strong>, Jirui Shi*, Shaoxun Wu*, Peiyan Gu*</strong><br>
                    <span>Final Project of CS181 Artificial Intelligence, 2021 Fall, ShanghaiTech University</span><br>
                    <a href="https://github.com/Adonis-galaxy/homepage/blob/main/research/AI_CS181_project.pdf">code</a>
                    <font size="2.5"><li>We aim deal with twitter emotion classification by implementing Naive Bayes-based methods and DNN based methods with some adaptations like TF-IDF.</li>
                      <li>We provide throughout ablation studies, visualizations, and error mode analysis. We obtained comparable results in this task with existing methods.</li></font>
                   </div>
                </div>
                <br>

                <!-- Temporarily removed -->
                <hr>
                <div class="resume-item d-flex flex-column flex-md-row">
                  <div class="resume-content mr-auto">
                    <img src="research/icon/MLFinal.png" alt="PontTrust" align="middle" style="width: 70%;display:block;margin-left: auto;margin-right: auto;">
                    <br />
                    <strong><papertitle class=blue><a href="research/ML_CS282_project.pdf">Generalized DUQ: Generalized Deterministic Uncertainty Quantification</a></papertitle></strong>
                    <br>Zhitong Gao*, <strong>Ziyao Zeng*</strong><br>
                    <span>Final Project of CS282 Machine Learning, 2021 Spring, ShanghaiTech University</span><br>
                    <font size="2.5"><li>We propose Generalized DUQ to extend the original uncertainty estimation of only utilizing its nearest centroid to utilize the nearest k centroids.</li>
                    <li>Compared with DUQ, we achieve a better generlization on Two Moons, FashionMNIST VS MNIST/NotMNIST, and CIFAR-10 VS SVHN.</li></font>
                  </div>
                </div>
                <br>

              <h5>2020</h5>

                <hr>
                <div class="resume-item d-flex flex-column flex-md-row">
                  <div class="resume-content mr-auto">
                    <img src="research/icon/DLFinal.png" alt="PontTrust" align="middle" style="width: 70%;display:block;margin-left: auto;margin-right: auto;">
                    <br />
                    <strong><papertitle class=blue><a href="research/DL_CS280_project.pdf">Seek Common while Shelving Differences: A New Way for dealing with Noisy Labels</a></papertitle></strong>
                    <br>Zhitong Gao*, <strong>Ziyao Zeng*</strong><br>
                    <span>Final Project of CS280 Deep Learning, 2020 Fall, ShanghaiTech University</span><br>
                    <font size="2.5"><li>We propose SCSD, which combines both the benefits of “cross training” and “agreement” by introducing a tri-net framework, to deal with learning with noisy labels. </li>
                    <li>Extensive experimental results on corrupted data from benchmark datasets including MNIST, CIFAR-10, CIFAR-100 demonstrate that SCSD is superior to many state-of-the-art approaches.</li></font>
                  </div>
                </div>
                <br>

          </div>

  <!-- Enterprise -->
  <div class="tab-pane fade" id="enterprise" role="tabpanel" aria-labelledby="enterprise-tab">
    <h4 style="margin-top: 10px">My Path towards Enterprise</h4>
  <p>
    <li>I minor at <a href=https://sem.shanghaitech.edu.cn/sem_en/2017/0724/c3725a29609/page.htm>Innovation and Entrepreneurship</a> at <a href="https://www.shanghaitech.edu.cn/eng/">ShanghaiTech University</a>, during which I attend <a href="https://www.babson.edu/academics/babson-academy/student-programs/entrepreneurial-mindshift/">Entrepreneurial Mindshift Program</a> of <a href="https://www.babson.edu/">Babson College</a>, and recieved its certification</li>

    <li>From a historical perspective, every leap of human society is brought by the transformation of productivity. The invention of the steamer released humans from toil, and the invention of the computer frees humans from boring work. I believe the next transformation of productivity would be the automation and intellectualization brought by AI technology. With more and more repetitive works being substituted by AI, humans could wipe out alienation completely, and liberate humans creativity and wisdom thoroughly.</li>
    
    <li>I believe AI technology can indeed contribute to every person's life for decades, not to mention the dramatic change brought by AGI. From another historical perspective, a lot of AI technologies come into being from applications in real industrial problems. Besides my research towards AGI in a theoretical way, as a starting AI scientist with an enterprise, it's my duty to carry out research based on real problems and apply research outcomes to realistic applications. In this way, I would like to explore the boundary of AI applications and the frontier of AGI knowledge, trying to make the world a better place.</li>

    <li>Towards this goal, being in this cosmopolitan city——Shanghai, I have been involved in various industrial exhibitions and entrepreneurship forums like <a href=http://www.visionchinashow.net>Vision China</a>, <a href=http://www.world-of-photonics-china.com.cn/zh-cn>Laser Photonics China</a>, and <a href=http://www.cme021.com>CME</a>, so as to catch up with contemporary AI application in industry, and think about what contribution I could bring to this field.</li>
  </p>


  </div>
          <!-- Adventure -->
          <div class="tab-pane fade" id="adventure" role="tabpanel" aria-labelledby="adventure-tab">
            <h4 style="margin-top: 10px">My Adventure</h4>
            
                  <li>I am a big fan of adventure who is enthusiastic about cycling, hiking and mountain climbing.</li>
                  <li>"Being a scientist and an adventurer has a lot of similarities, they both want to achieve something that hasn't been achieved before."</li>
                  <li>In 2015, I have hiked across Lake District of England in 1 week.</li>
                  <img style="width:32%;max-width:100%" align="middle" src="images/lakedistrict/DSC_0015.JPG">
                  <img style="width:32%;max-width:100%" align="middle" src="images/lakedistrict/DSC_1167.JPG">
                  <img style="width:32%;max-width:100%" align="middle" src="images/lakedistrict/DSC_1442.JPG">
                  <li>In 2019, I have cycled cross Tibet for 28 days from <a href="https://sco.wikipedia.org/wiki/Chengdu">Chengdu</a> to <a href="https://de.wikipedia.org/wiki/Lhasa">Lhasa</a> for 2135 km.</li>
                  <p></p>
                  <img style="width:97%;max-width:100%" align="middle" src="images/qixingluxian.jpg">
                  <img style="width:32%;max-width:100%" align="middle" src="images/zhongdian.jpg">
                  <img style="width:32%;max-width:100%" align="middle" src="images/mila.jpg">
                  <img style="width:32%;max-width:100%" align="middle" src="images/chuanzang3.jpg">
                  <img style="width:24%;max-width:100%" align="middle" src="images/bingchuan.jpg">
                  <img style="width:24%;max-width:100%" align="middle" src="images/3838.jpg">
                  <img style="width:24%;max-width:100%" align="middle" src="images/chuanzang1.jpg">
                  <img style="width:24%;max-width:100%" align="middle" src="images/dongda.jpg">
                  <p></p>
                  <li>In 2022, I have cycled cross Tibet and Xinjiang for 1 month from <a href=https://en.wikipedia.org/wiki/%C3%9Cr%C3%BCmqi">Ürümqi</a> to <a href="https://de.wikipedia.org/wiki/Lhasa">Lhasa</a> for 5000 km, with about 2000 km cycling at an average altitude of 4500 m.</li>
                  <p></p>
                  <img style="width:32%;max-width:100%" align="middle" src="images/xinzangxian/plan.jpg">
                  <img style="width:32%;max-width:100%" align="middle" src="images/xinzangxian/qidian.jpg">
                  <img style="width:32%;max-width:100%" align="middle" src="images/xinzangxian/binhe.jpg">
                  <img style="width:32%;max-width:100%" align="middle" src="images/xinzangxian/shanding.jpg">
                  <img style="width:32%;max-width:100%" align="middle" src="images/xinzangxian/swim.jpg">
                  <img style="width:32%;max-width:100%" align="middle" src="images/xinzangxian/zhufeng.jpg">

                  <img style="width:24%;max-width:100%" align="middle" src="images/xinzangxian/zhongdian.jpg">
                  <img style="width:24%;max-width:100%" align="middle" src="images/xinzangxian/daban.jpg">
                  <img style="width:24%;max-width:100%" align="middle" src="images/xinzangxian/gangrenboqi.jpg">                                
                  <img style="width:24%;max-width:100%" align="middle" src="images/xinzangxian/zhuanshan.jpg">
                  <img style="width:24%;max-width:100%" align="middle" src="images/xinzangxian/zipai.jpg">
                  
                  <img style="width:24%;max-width:100%" align="middle" src="images/xinzangxian/lasa.jpg">
                  <img style="width:24%;max-width:100%" align="middle" src="images/xinzangxian/zhengshuheying.jpg">
                  <img style="width:24%;max-width:100%" align="middle" src="images/xinzangxian/zhengshu.jpg">
                  <p></p>

                  <p></p>
                  <li>My other photos regarding adventures.</li>
                  <img style="width:24%;max-width:100%" align="middle" src="images/zengzy.jpg">
                  <img style="width:24%;max-width:100%" align="middle" src="images/cover_full_body.jpg">
                  <img style="width:24%;max-width:100%" align="middle" src="images/dongshan.jpg.jpg">
                  <p></p>
                  


          </div>
          <!-- Others about me -->
      <div class="tab-pane fade" id="others" role="tabpanel" aria-labelledby="others-tab">
            <h4 style="margin-top: 10px">Other things about myself</h4>
            <p>
              <li>I'm a disciplined body builder.</li>
              <li>"Discipline is doing what you hate to do, but do it as you love it."</li>
              <li>I always enforce such discipline not only into my body-building but my work and life.</li>
              <img style="width:24%;max-width:100%" align="middle" src="images/body.jpg">
            </p>
            <hr /> <!-- 画一条横线-->
            <p> 
              <li>Besides, I'm an amateur sci-fi novelist.</li>
              <li>I continuously think about the formation of technology and society in the future, especially their influence on human beings. With the development of AI technology, the field of careers will be totally shaped within a few decades, which is related to everyone's benefit. It's my duty to foresee this transformation and try my best to turn it into a good way. As a sci-fi lover, my favoriate novelist is <a href="https://en.wikipedia.org/wiki/Isaac_Asimov">Isaac Asimov</a> with his great work <a href="https://en.wikipedia.org/wiki/Foundation_(Asimov_novel)">Foundation</a></li>
              <li><a href="https://github.com/Adonis-galaxy/Adonis-Sci-Fi-Novel">Link to my sci-fi works</a>
            </p>
            <hr /> <!-- 画一条横线-->
            <p> 
              <li>I'm an amateur Unity game developer, previous supervised by <a href="http://www.briancox.be/?page=home">Brain Cox</a>, screenshots of my previous works have been shown below.</li>
              <li>Snow Ranger</li>
              <img style="max-width:50%" align="middle" src="images/Unity_HW2.png">
              <li><a href="https://www.bilibili.com/video/BV1WS4y1Z7rz?p=1&share_medium=ipad&share_plat=ios&share_source=QQ&share_tag=s_i&timestamp=1641184133&unique_k=GHTLVUs">Darkside</a></li>
              <img style="max-width:33%" align="middle" src="images/main_menu.jpg">
              <img style="max-width:33%" align="middle" src="images/night_vision.jpg">
              <img style="max-width:33%" align="middle" src="images/puzzle.jpg">

            </p>
            <hr /> <!-- 画一条横线-->
            <p> 
              <li>I'm also an amateur composer, conducter, pianist, trombone player.</li>
              <li>I have been playing Tarot since 2014, familiar with Thoth and Shadowflower, dedicated to combining Tarot with modern psychology to serve as a tool for consciousness.</li>
            </p>
            <hr /> <!-- 画一条横线-->
            <p> 
              <li>I'm excited about all kinds of voluntary especially those related to environment protection.</li>
              <li>I believe it's our instinctive duty to preserve the integrity of the earth (at least until we could immigrate to other planets).</li>
              <li>Currently, I'm volunteering at <a href="http://www.wwfchina.org/">WWF-China</a> and <a href="http://www.greenpeace.org.cn/">Greenpeace</a></li>
            </p>
        </div>

          <!-- 结尾功能组件 -->
        </div>
      </div>
    <br>
    <br>

    <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>

  </body>

</html>
