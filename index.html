
<!DOCTYPE html>
<html lang="en">

  <head>

    <meta charset="utf-8">

    <title>Xingyi Zhou</title>

    <!-- Bootstrap core CSS -->
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">

    <!-- Custom styles for this template -->
    <link href="css/style.css" rel="stylesheet">
  </head>

  <body>

    <div class="container col-xs-12 col-sm-8 col-md-8 col-lg-8 col-md-offset-2 col-lg-offset-2 col-sm-offset-2 p-0">

        <h1 id = "myName">Xingyi 
            <span class="text-primary">Zhou</span>
        </h1>

        <ul class="nav nav-tabs" id="myTab" role="tablist">
        <li class="nav-item">
          <a class="nav-link active" id="home-tab" data-toggle="tab" href="#home" role="tab" aria-controls="home" aria-selected="true">Home</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" id="publication-tab" data-toggle="tab" href="#publication" role="tab" aria-controls="publication" aria-selected="false">Research</a>
        </li>
      </ul>
      <div class="tab-content" id="nav-tabContent">
    <div class="tab-pane fade show active" id="home" role="tabpanel" aria-labelledby="home-tab">

        <table>
          <tr>
            <td width = "62%">
              <p align>I am a fourth year (2017-) Computer Science Ph.D. student at The University of Texas at Austin, supervised by Prof. <a href="http://www.philkr.net/">Philipp Kr&auml;henb&uuml;hl</a>. I obtained my bachelor degree from School of Computer Science at Fudan University, advised by Prof. <a href="http://weizh2016.github.io/">Wei Zhang</a> and Prof. <a href="https://scholar.google.com.hk/citations?user=DTbhX6oAAAAJ">Xiangyang Xue</a>. </p>
              <p>I have interned with Dr. <a href="https://yichenwei.github.io/">Yichen Wei</a> at Microsoft Research Asia, <a href="https://ai.google/research/people/TylerZhu">Tyler Zhu</a> and Dr. <a href="https://www.cs.ubc.ca/~murphyk/">Kevin Murphy</a> at Google Research, and Dr. <a href="http://vladlen.info/">Vladlen Koltun</a> at Intel Lab. I am a recipient of <a href="https://research.fb.com/fellows/?dateYear=2021">Facebook Fellowship</a> in 2021.</p> 
			  <p>My research focuses on object-level visual recognition, including object detection, 3D perception, pose estimation, and tracking.</p>
                <a href="materials/XingyiZhou-CV.pdf" target="_blank">CV</a> / <a href="https://scholar.google.com/citations?user=47n-0mwAAAAJ&hl=en">Google Scholar</a> / <a href="https://github.com/xingyizhou">GitHub</a> / <a href="https://www.linkedin.com/in/xingyi-zhou-21925290/">LinkedIn</a>

            </td>
            <td width = "1%">
            </td>
            <td width = "30%">

              <img src="materials/profile.jpg" alt="PontTrust" align="middle" style="width: 100%;">
            </td>
        </tr>
        
        </table>
        <br>
        <p id = "linkToJiarui">The profile picture is taken by my lovely girlfriend <a href="http://jiaruigao.com" >Jiarui Gao</a>.</p>
        <p align="center"> Last updated April 2021</p>
        </div>
        
        <div class="tab-pane fade" id="publication" role="tabpanel" aria-labelledby="publication-tab">
		
          <div class="resume-item d-flex flex-column flex-md-row">
            <div class="resume-content mr-auto">
              <h4 style="margin-top: 10px">Overview</h4>
              <hr>
              <p>Computer vision is fractured into datasets, and split into a multitude of visual domains. My research aims to remove the artificial barriers of datasets and make computer vision work in the wild. Towards this goal, my research focuses on two components: unified point-based objects representations, and a framework to automatically unify taxonomies of multiple datasets. I developed a point-based detection framework, <a href="https://github.com/xingyizhou/CenterNet" target="_blank">CenterNet</a>, that unifies many object-based recognition tasks, including object detection, human pose estimation, <a href="https://github.com/xingyizhou/CenterTrack" target="_blank">tracking</a>, and <a href="https://github.com/tianweiy/CenterPoint" target="_blank">3D detection</a>. The CenterNet framework forms the basis of a family of detection models, and is already widely used. In our recent work <a href="https://github.com/xingyizhou/UniDet" target="_blank">UniDet</a>, we explored how to learn object-based recognition from multiple data sources. It merges taxonomies across different datasets using a completely visual distance metric. Our entry won the <a href="http://www.robustvision.net/index.php" target="_blank">ECCV 2020 Robust Vision Challenge</a>. Going forward, I want to unify all object-based computer vision tasks in a single model, by jointly training on multiple datasets with different task annotations --- there will be one computer vision model instead of a zoo of domain-specific ones. </p>
              <br>
			  
		<h4>2021</h4>
		<hr>
			  
          <div class="resume-item d-flex flex-column flex-md-row">
            <div class="resume-content mr-auto">
              <img src="materials/CenterNet2.jpg" alt="PontTrust" align="middle" style="width: 70%;display:block;margin-left: auto;margin-right: auto;">
              <br />
              <papertitle class=blue><a href="https://arxiv.org/abs/2103.07461">Probabilistic two-stage detection</a></papertitle>
              <br><strong>Xingyi Zhou</strong>, Vladlen Koltun, Philipp Kr&auml;henb&uuml;hl<br>
              <span>arXiv technical report, 2021</span><br>
              <i class=blue><a href="materials/CenterNet2.bib">bibtex</a> &nbsp/&nbsp</i>
              <i class=blue><a href="https://github.com/xingyizhou/CenterNet2" target="_blank">code</a>&nbsp/&nbsp</i>
              <i class=blue><a href="https://github.com/xingyizhou/CenterNet2/blob/master/projects/CenterNet2/centernet2_docs/MODEL_ZOO.md" target="_blank">models</a></i>
            </div>
          </div>
          <br>
		  
          <div class="resume-item d-flex flex-column flex-md-row">
            <div class="resume-content mr-auto">
              <img src="materials/UniDet.jpg" alt="PontTrust" align="middle" style="width: 70%;display:block;margin-left: auto;margin-right: auto;">
              <br />
              <papertitle class=blue><a href="https://arxiv.org/abs/2102.13086">Simple multi-dataset detection</a></papertitle>
              <br><strong>Xingyi Zhou</strong>, Vladlen Koltun, Philipp Kr&auml;henb&uuml;hl<br>
              <span>arXiv technical report, 2021</span><br>
              <i class=blue><a href="materials/UniDet.bib">bibtex</a> &nbsp/&nbsp</i>
              <i class=blue><a href="https://github.com/xingyizhou/UniDet" target="_blank">code</a>&nbsp/&nbsp</i>
              <i class=blue><a href="https://github.com/xingyizhou/UniDet/blob/master/projects/UniDet/unidet_docs/REPRODUCE.md" target="_blank">models</a></i>
            </div>
          </div>
          <br>
			  
			  
          <div class="resume-item d-flex flex-column flex-md-row">
            <div class="resume-content mr-auto">
              <img src="materials/CenterPoint.png" alt="PontTrust" align="middle" style="width: 50%;display:block;margin-left: auto;margin-right: auto;">
              <br />
              <papertitle class=blue><a href="https://arxiv.org/abs/2006.11275">Center-based 3D Object Detection and Tracking</a></papertitle>
              <br>Tianwei Yin, <strong>Xingyi Zhou</strong>, Philipp Kr&auml;henb&uuml;hl<br>
              <span>Computer Vision and Pattern Recognition (CVPR), 2021</span><br>
              <i class=blue><a href="materials/TianweiYin-cvpr2021.bib">bibtex</a> &nbsp/&nbsp</i>
              <i class=blue><a href="https://github.com/tianweiy/CenterPoint" target="_blank">code</a>&nbsp/&nbsp</i>
              <i class=blue><a href="https://github.com/tianweiy/CenterPoint/blob/master/docs/MODEL_ZOO.md" target="_blank">models</a></i>
            </div>
          </div>
          <br>
			  
        <h4>2020</h4>
        <hr>
			  
          <div class="resume-item d-flex flex-column flex-md-row">
            <div class="resume-content mr-auto">
              <img src="materials/CenterTrack.png" alt="PontTrust" align="middle" style="width: 70%;display:block;margin-left: auto;margin-right: auto;">
              <br />
              <papertitle class=blue><a href="http://arxiv.org/abs/2004.01177">Tracking Objects as Points</a></papertitle>
              <br><strong>Xingyi Zhou</strong>, Vladlen Koltun, Philipp Kr&auml;henb&uuml;hl<br>
              <span>ECCV, 2020 (Spotlight)</span><br>
              <i class=blue><a href="materials/XingyiZhou-ECCV2020.bib">bibtex</a> &nbsp/&nbsp</i>
              <i class=blue><a href="https://github.com/xingyizhou/CenterTrack" target="_blank">code</a>&nbsp/&nbsp</i>
              <i class=blue><a href="https://github.com/xingyizhou/CenterTrack/blob/master/readme/MODEL_ZOO.md" target="_blank">model zoo</a></i>
            </div>
          </div>
          <br>
			  
			  
		<h4>2019</h4>
		<hr>
			  
          <div class="resume-item d-flex flex-column flex-md-row">
            <div class="resume-content mr-auto">
              <img src="materials/arXiv2019.jpg" alt="PontTrust" align="middle" style="width: 70%;display:block;margin-left: auto;margin-right: auto;">
              <br />
              <papertitle class=blue><a href="https://arxiv.org/abs/1904.07850">Objects as Points</a></papertitle>
              <br><strong>Xingyi Zhou</strong>, Dequan Wang, Philipp Kr&auml;henb&uuml;hl<br>
              <span>arXiv technical report, 2019</span><br>
              <i class=blue><a href="materials/XingyiZhou-arXiv2019.bib">bibtex</a> &nbsp/&nbsp</i>
              <i class=blue><a href="https://github.com/xingyizhou/CenterNet" target="_blank">code</a>&nbsp/&nbsp</i>
              <i class=blue><a href="https://github.com/xingyizhou/CenterNet/blob/master/readme/MODEL_ZOO.md" target="_blank">model zoo</a></i>
            </div>
          </div>
          <br>

          <div class="resume-item d-flex flex-column flex-md-row">
            <div class="resume-content mr-auto">
              <img src="materials/CVPR2019.jpg" alt="PontTrust" align="middle" style="width: 70%;display:block;margin-left: auto;margin-right: auto;">
              <br />
              <papertitle class=blue><a href="https://arxiv.org/abs/1901.08043">Bottom-up Object Detection by Grouping Extreme and Center Points</a></papertitle>
              <br><strong>Xingyi Zhou</strong>, Jiacheng Zhuo, Philipp Kr&auml;henb&uuml;hl<br>
              <span>Computer Vision and Pattern Recognition (CVPR), 2019</span><br>
              <i class=blue><a href="materials/XingyiZhou-CVPR2019.bib">bibtex</a> &nbsp/&nbsp</i>
              <i class=blue><a href="https://github.com/xingyizhou/ExtremeNet" target="_blank">code</a>&nbsp/&nbsp</i>
              <i class=blue><a href="https://drive.google.com/open?id=1re-A74WRvuhE528X6sWsg1eEbMG8dmE4" target="_blank">model</a>&nbsp/&nbsp</i>
              <i class=blue><a href="https://drive.google.com/open?id=1te-CHOVpW550TuCHUYQQyOHzEKPHkGXe" target="_blank">supplementary</a></i>
            </div>
          </div>
          <br>
          

          <h4>2018</h4>
              <hr>
          <div class="resume-item d-flex flex-column flex-md-row">
            <div class="resume-content mr-auto">
              <img src="materials/arXiv2018.jpg" alt="PontTrust" align="middle" style="width: 70%;display:block;margin-left: auto;margin-right: auto;">
              <br />
              <papertitle class=blue><a href="https://arxiv.org/abs/1803.09331">StarMap for Category-Agnostic Keypoint and Viewpoint Estimation</a></papertitle>
              <br><strong>Xingyi Zhou</strong>, Arjun Karpur, Linjie Luo, Qixing Huang<br>
              <span>European Conference on Computer Vision (ECCV), 2018</span><br>
              <i class=blue><a href="materials/XingyiZhou-arXiv2018.bib">bibtex</a> &nbsp/&nbsp</i>
              <i class=blue><a href="https://github.com/xingyizhou/StarMap" target="_blank">code</a>&nbsp/&nbsp</i>
              <i class=blue><a href="https://drive.google.com/file/d/1bwCeC4F0OLFYceiaAuUGB6pU8OOZor1k/view?usp=sharing" target="_blank">model</a>&nbsp/&nbsp</i>
              <i class=blue><a href="https://drive.google.com/file/d/1IEcHBdQ8u2HTKiNz88ItJWDRQUOJnf60/view?usp=sharing" target="_blank">supplementary</a>&nbsp/&nbsp</i>
              <i class=blue><a href="materials/ECCV18_starmap_poster.pdf" target="_blank">poster</a></i>
            </div>
          </div>
          <br>

          <div class="resume-item d-flex flex-column flex-md-row">
            <div class="resume-content mr-auto">
              <img src="materials/arXiv2017_2.png" alt="PontTrust" align="middle" style="width: 70%;display:block;margin-left: auto;margin-right: auto;">
              <br />
              <papertitle class=blue><a href="https://arxiv.org/abs/1712.05765">  Unsupervised Domain Adaptation for 3D Keypoint Estimation via View Consistency</a></papertitle>
              <br><strong>Xingyi Zhou</strong>, Arjun Karpur, Chuang Gan, Linjie Luo, Qixing Huang<br>
              <span>European Conference on Computer Vision (ECCV), 2018</span><br>
              <i class=blue><a href="materials/XingyiZhou-arXiv2017-2.bib">bibtex</a> &nbsp/&nbsp</i>
              <i class=blue><a href="https://github.com/xingyizhou/3DKeypoints-DA" target="_blank">code</a>&nbsp/&nbsp</i>
              <i class=blue><a href="https://drive.google.com/file/d/1nXNPHr8UffI79yT0fBPOy-mTb5iqoYe6/view" target="_blank">model</a>&nbsp/&nbsp</i>
              <i class=blue><a href="materials/ECCV18_da_poster.pdf" target="_blank">poster</a></i>
            </div>
          </div>
          <br>
          <h4>2017</h4>
              <hr>

          <div class="resume-item d-flex flex-column flex-md-row">
            <div class="resume-content mr-auto">
              <img src="materials/XingyiZhou-ICCV2017.jpg" alt="PontTrust" align="middle" style="width: 70%;display:block;margin-left: auto;margin-right: auto;">
              <br />
              <papertitle class=blue><a href="https://arxiv.org/abs/1704.02447">Towards 3D Human Pose Estimation in the Wild: A weakly-supervised Approach</a></papertitle>
              <br><strong>Xingyi Zhou</strong>, Qixing Huang, Xiao Sun, Xiangyang Xue, Yichen Wei<br>
              <span>International Conference on Computer Vision (ICCV), 2017</span><br>
              <i class=blue><a href="materials/XingyiZhou-arXiv2017.bib">bibtex</a> &nbsp/&nbsp</i>
              <i class=blue><a href="https://github.com/xingyizhou/pose-hg-3d" target="_blank">code (torch)</a>&nbsp/&nbsp</i>
              <i class=blue><a href="https://github.com/xingyizhou/Pytorch-pose-hg-3d" target="_blank">code (PyTorch)</a>&nbsp/&nbsp</i>
              <i class=blue><a href="http://xingyizhou.xyz/hgreg-3d.t7" target="_blank">model</a>&nbsp/&nbsp</i>
              <i class=blue><a href="materials/XingyiZhou-arXiv2017-Supplementary.pdf" target="_blank">supplementary</a>&nbsp/&nbsp</i>
              <i class=blue><a href="materials/ICCV17-poster.pdf" target="_blank">poster</a></i>
            </div>
          </div>
          <br>
          <h4>2016</h4>
              <hr>

          <div class="resume-item d-flex flex-column flex-md-row ">
            <div class="resume-content mr-auto">
              <img src="materials/XingyiZhou-GMDL2016.png" alt="PontTrust" align="middle" style="width: 70%;display:block;margin-left: auto;margin-right: auto;">
              <br />
              <papertitle class=blue><a href="http://arxiv.org/abs/1609.05317">Deep Kinematic Pose Regression</a></papertitle>
              <br><strong>Xingyi Zhou</strong>, Xiao Sun, Wei Zhang, Shuang Liang, Yichen Wei<br>
              <span>ECCV Workshop on Geometry Meets Deep Learning, 2016</span><br>
              <i class=blue><a href="materials/XingyiZhou-GMDL2016.bib">bibtex</a> &nbsp/&nbsp</i>
              <i class=blue><a href="materials/ECCVW16/ECCVW16_src.zip">code</a>&nbsp/&nbsp</i>
              <i class=blue><a href="materials/ECCVW16/Model.caffemodel">model</a>&nbsp/&nbsp</i>
              <i class=blue><a href="materials/XingyiZhou-GMDL2016-Poster.pdf" target="_blank">poster</a></i>
            </div>
          </div>
          <br>

          <div class="resume-item d-flex flex-column flex-md-row ">
            <div class="resume-content mr-auto">
              <img src="materials/XingyiZhou-IJCAI2016.png" alt="PontTrust" align="middle" style="width: 70%;display:block;margin-left: auto;margin-right: auto;">
              <br />
              <papertitle class=blue><a href="https://arxiv.org/abs/1606.06854">Model-based Deep Hand Pose Estimation</a></papertitle>
              <br><strong>Xingyi Zhou</strong>, Qingfu Wan, Wei Zhang, Xiangyang Xue, Yichen Wei<br>
              <span>International Joint Conference on Artificial Intelligence  (IJCAI), 2016</span><br>
              <i class=blue><a href="materials/XingyiZhou-IJCAI2016.bib">bibtex</a> &nbsp/&nbsp</i>
              <i class=blue><a href="https://github.com/xingyizhou/DeepModel" target="_blank">code</a>&nbsp/&nbsp</i>
              <i class=blue><a href="materials/XingyiZhou-IJCAI2016-Slides.pdf" target="_blank">slides</a>&nbsp/&nbsp</i>
              <i class=blue><a href="materials/XingyiZhou-IJCAI2016-Poster.pdf" target="_blank">poster</a></i>
              
            </div>
          </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
    <br>
    <br>

    <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>

  </body>

</html>
