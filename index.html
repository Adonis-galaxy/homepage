
<!DOCTYPE html>
<html lang="en">
  
  <head>
    <meta charset="utf-8">
    <link rel="icon" type="image/png" href="images/3838_icon.png">
  	<meta charset="utf-8">

    <style type="text/css">
      a:link {
        color:rgb(0, 47, 164);
        text-decoration:none;
      }
      a:visited {
        color:rgb(0, 47, 164);
        text-decoration:none;
      }
      a:hover {
        color:rgb(0, 47, 164);
        text-decoration:none;
      }
      a:active {
        color:rgb(0, 47, 164);
        text-decoration:none;
      }
      
      
    </style>
    <title>Ziyao Zeng</title>

    <!-- Bootstrap core CSS -->
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">

    <!-- Custom styles for this template -->
    <link href="css/style.css" rel="stylesheet">
  </head>

  <body>

    <div class="container col-xs-12 col-sm-8 col-md-8 col-lg-8 col-md-offset-2 col-lg-offset-2 col-sm-offset-2 p-0">

        <h1 id = "myName" align ="center">Ziyao
            <span class="text-primary">Zeng</span> (Adonis)
        </h1>
<!-- 按钮设置 -->
        <ul class="nav nav-tabs" id="myTab" role="tablist">
        <li class="nav-item">
          <a class="nav-link active" id="home-tab" data-toggle="tab" href="#home" role="tab" aria-controls="home" aria-selected="true">Home</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" id="publication-tab" data-toggle="tab" href="#publication" role="tab" aria-controls="publication" aria-selected="false">Research</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" id="enterprise-tab" data-toggle="tab" href="#enterprise" role="tab" aria-controls="enterprise" aria-selected="false">Enterprise</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" id="adventure-tab" data-toggle="tab" href="#adventure" role="tab" aria-controls="adventure" aria-selected="false">Adventure</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" id="others-tab" data-toggle="tab" href="#others" role="tab" aria-controls="others" aria-selected="false">Others</a>
        </li>
      </ul>
      <div class="tab-content" id="nav-tabContent">
<!-- 正文 -->
  <!-- Introduction -->
    <div class="tab-pane fade show active" id="home" role="tabpanel" aria-labelledby="home-tab">

     
        <table>
          <tr>
            <td width = "62%">
              <p align>    
                <!-- <h4 style="margin-top: 10px">Starting AI-Scientist with Enterprise</h4>    -->

                I'm an incoming Ph.D. student in Computer Science (2023 - [Expected] 2029) at <a href="https://www.yale.edu/">Yale University</a>, supervised by <a href="https://www.cs.yale.edu/homes/wong-alex/">Prof. Alex Wong</a>. Previous to that, I would obtain my B.Eng. in Computer Science (2019 - [Expected] 2023) at <a href="https://www.shanghaitech.edu.cn/eng/">ShanghaiTech University</a>.
                
                <p></p>

                I am currently interning with <a href="https://www.cis.upenn.edu/~jshi/">Prof. Jianbo Shi</a> at <a href="https://www.grasp.upenn.edu/">UPenn GRASP Lab</a>. 
                Previously, I interned with <a href="https://xmhe.bitbucket.io/">Prof. Xuming He</a> at <a href="http://plus.sist.shanghaitech.edu.cn/">ShanghaiTech PLUS Group</a>.    
                I have served as a reviewer of <a href="https://cvpr2022.thecvf.com/">CVPR 2022</a> and <a href="https://iccv2023.thecvf.com/">ICCV 2023</a>. 

                <p></p>
                I conduct research on <strong>Multimodality Perception</strong> inspired by human perception. Currently, my research interest mainly covers <strong>3D Perception</strong> and <strong>Vision-Language Models</strong>.
                <p></p>
                <a href="CV_Ziyao Zeng.pdf">CV</a> /
                <!-- <a href="Long CV-Ziyao Zeng.pdf">Long CV</a> / -->
                <a href="https://scholar.google.com/citations?hl=en&user=FYL2DYEAAAAJ">Google Scholar</a> /
                <a href="https://www.linkedin.com/in/ziyaozeng/">LinkedIn</a> / 
                <a href="https://github.com/Adonis-galaxy">GitHub</a> /
                <a href="http://plus.sist.shanghaitech.edu.cn/author/ziyao-zeng/">PLUS Group</a>  
                <p></p>
                Email: ziyao.zeng@@yale.edu, zengzy@@shanghaitech.edu.cn
                
                
            </td>
            <td width = "1%">
            </td>
            <td width = "30%">
              <img src="images/cover_full_body.jpg" alt="PontTrust" align="middle" style="width: 100%;">
            </td>
        </tr>
        
        </table>
        <br>
        <p>I am looking for a Ph.D position starting at fall 2023.</p>
        <p id = "linkToJiarui">Website format from <a href="https://www.cs.utexas.edu/~zhouxy/#publication">Xingyi Zhou</a>.</p>
        <p align="center"> Last updated Sept. 2022</p>  <!-- Time -->
        </div>
        
        <!-- Research -->
        <div class="tab-pane fade" id="publication" role="tabpanel" aria-labelledby="publication-tab">
              <h4 style="margin-top: 10px">Research Overview</h4>
              <p>
                <li>Since the age of 13, deeply touched by <i><a href="https://en.wikipedia.org/wiki/Foundation_(Asimov_novel)">Foundation</a></i> by <a href="https://en.wikipedia.org/wiki/Isaac_Asimov">Isaac Asimov</a>, my dream has been to explore the galaxy. Unable to fulfill this mission with existing technology, I reoriented my goal to create an AI who can think like humans (just like the dream of <a href="https://people.idsia.ch//~juergen/">Prof. Jürgen Schmidhuber</a>), which we call Artificial General Intelligence (AGI) today, and explore the galaxy together. When humans perceive the surrounding environment, we see (2D vision), touch (3D vision), and hear (audio) simultaneously to understand (language). Therefore, AGI’s understanding should likewise be based upon the complementary perception of different modalities. Toward this goal, I conduct research on <strong>Multimodality Perception</strong> inspired by human perception. Currently, my research interest primarily lies in <strong>3D Perception</strong> and <strong>Vision-Language Models</strong>.

                <p></p>
                <li>Recently, human-like AI perception has come closer to reality. Large-scale foundation models pre-trained with multimodal data provide promising unified frameworks for Multimodal Perception. In particular, Contrastive Language-Image Pretraining  (CLIP) trains both an image and text encoder, and conducts contrastive learning in feature space. It simulates the process of parents pointing to objects to assist children in recognizing them. In this simple relationship, children not only learn to classify objects but also to segment them without any pixel-wise mask annotations or box annotations. In light of this insight, I have become passionate about realizing the perception potential of CLIP for other visual tasks.</li>
                <p></p>
                <li><a href="https://arxiv.org/abs/2207.01077">DepthCLIP</a>, which I directed as a co-first author, expanding upon CLIP, reveals that humans learn to predict depth not by pixel-wise depth annotation, but by relative depth semantics. A child is taught “this tree is far, and that bus is close.” Consequently, he or she builds a semantic depth understanding of seen monocular images, indicating which object is near, and which object is far. Together with my collaborators, I found that CLIP also learns from a mutual understanding of semantic language-image concepts, and thus has the same ability to distinguish relative depth as humans. Given such insight, to conduct zero-shot training-free monocular depth estimation, I leveraged CLIP to obtain the semantic depth response of monocular images, then designed a projection scheme to obtain the quantified depth prediction. Surprisingly, DepthCLIP surpasses most existing unsupervised methods and even approaches early fully-supervised networks, which demonstrates that AI could acquire competent perception by simulating human perception. </li>
          
                <p></p>
              </p>

              <!-- Publications -->
              <h4>Publications</h4>
              (* indicates equal contributions, † indicates the corresponding author)
              <h5>2022</h5>
              <hr>
              <div class="resume-item d-flex flex-column flex-md-row">
                <div class="resume-content mr-auto">
                  <img src="research/icon/PointCLIPv2.png" alt="PontTrust" align="middle" style="width: 70%;display:block;margin-left: auto;margin-right: auto;">
                  <br />
                  <strong><papertitle class=blue><a href="https://arxiv.org/abs/2211.11682">PointCLIP V2: Adapting CLIP for Powerful 3D Open-world Learning</a></papertitle></strong>
                  <br>Xiangyang Zhu*, Renrui Zhang*, Bowei He, <strong>Ziyao Zeng</strong>, Shanghang Zhang, Peng Gao<br>
                  <span>arXiv technical report, 2022, submitted to a top-tier conference</span><br>
                  <a href="https://github.com/yangyangyang127/PointCLIP_V2">code</a>
                  <font size="2.5"><li>We introduce a realistic shape projection module, and leverage large-scale language models to automatically design 3D-semantic prompt.</li>
                  <li>Our approach significantly surpasses PointCLIP by +42.90%, +40.44%, and +28.75% accuracy on three datasets for zero-shot 3D classification.</li></font>
                </div>
              </div>
              <br>


              <hr>
              <div class="resume-item d-flex flex-column flex-md-row">
                <div class="resume-content mr-auto">
                  <img src="research/icon/iQuery.png" alt="PontTrust" align="middle" style="width: 70%;display:block;margin-left: auto;margin-right: auto;">
                  <br />
                  <strong><papertitle class=blue><a href="https://arxiv.org/abs/2212.03814">iQuery: Instruments as Queries for Audio-Visual Sound Separation</a></papertitle></strong>
                  <br>Jiaben Chen, Renrui Zhang, Dongze Lian, Jiaqi Yang, <strong>Ziyao Zeng</strong>, Jianbo Shi<br>
                  <span>Accepted by CVPR 2023</span><br>
                  <a href="https://github.com/JiabenChen/iQuery">code</a>
                  <font size="2.5"><li>We re-formulate visual-sound separation task and propose Instrument as Query (iQuery) with a flexible query expansion mechanism.</li>
                  <li>We demonstrate state-of-the-art performance, with up to 44.2 % improvement of SDR on MUSIC benchmark.</li></font>
                </div>
              </div>
              <br>




              <hr>
              <div class="resume-item d-flex flex-column flex-md-row">
                <div class="resume-content mr-auto">
                  <img src="research/icon/depthclip.png" alt="PontTrust" align="middle" style="width: 70%;display:block;margin-left: auto;margin-right: auto;">
                  <br />
                  <strong><papertitle class=blue><a href="https://arxiv.org/abs/2207.01077">Can Language Understand Depth?</a></papertitle></strong>
                  <br>Renrui Zhang*, <strong>Ziyao Zeng*</strong>, Ziyu Guo, Yafeng Li<br>
                  <span>Accepted by ACM Multimedia 2022 as Brave New Idea (Accepte Rate=12.5%)</span><br>
                  <a href="https://github.com/Adonis-galaxy/DepthCLIP">code</a>
                  <font size="2.5"><li>We are the first to conduct zero-shot adaptation from the semantic language knowledge to quantified downstream vision tasks and perform zero-shot training-free monocular depth estimation. </li>
                  <li>Our DepthCLIP surpasses existing unsupervised methods and even approaches the early fully-supervised networks. </li></font>
                </div>
              </div>
              <br>



              <h5>2021</h5>
                <hr>
                <div class="resume-item d-flex flex-column flex-md-row">
                  <div class="resume-content mr-auto">
                    <img src="research/icon/dspoint.png" alt="PontTrust" align="middle" style="width: 70%;display:block;margin-left: auto;margin-right: auto;">
                    <br />
                    <strong><papertitle class=blue><a href="https://arxiv.org/abs/2111.10332">DSPoint: Dual-scale Point Cloud Recognition with High-frequency Fusion</a></papertitle></strong>
                    <br>Renrui Zhang*, <strong>Ziyao Zeng*</strong>, Ziyu Guo, Xinben Gao, Kexue Fu, Jianbo Shi† <br>
                    <span>arXiv technical report, 2021</span><br>
                    <a href="https://github.com/Adonis-galaxy/DSPoint">code</a>
                    <font size="2.5"><li>To deal with point cloud processing, we proposed DSPoint to conduct dual-scale processing: one by point-wise convolution for fine-grained geometry parsing, the other by voxel-wise global attention for long-range structural exploration.</li>
                    <li>We design a co-attention fusion module for feature alignment to blend local-global modalities, which conducts inter-scale cross-modality interaction by communicating high-frequency coordinates information.</li></font>
                  </div>
                </div>
                <br>

                
                <hr>
                <div class="resume-item d-flex flex-column flex-md-row">
                  <div class="resume-content mr-auto">
                    <img src="research/icon/vtclip.png" alt="PontTrust" align="middle" style="width: 70%;display:block;margin-left: auto;margin-right: auto;">
                    <br />
                    <strong><papertitle class=blue><a href="https://arxiv.org/abs/2112.02399">VT-CLIP: Enhancing Vision-Language Models with Visual-guided Texts</a></papertitle></strong>
                    <br>Longtian Qiu, Renrui Zhang, Ziyu Guo, <strong>Ziyao Zeng</strong>, Yafeng Li, Guangnan Zhang<br>
                    <span>arXiv technical report, 2021</span><br>
                    <font size="2.5"><li>To improve downstream adaptation of Contrastive Vision-Language Pre-training (CLIP), we propose VT-CLIP to enhance vision-language modeling via visual-guided texts. </li>
                    <li>Specifically, we guide the text feature to adaptively explore informative regions on the image and aggregate the visual feature by cross-attention.</li></font>
                  </div>
                </div>
                <br>


                <hr>
                <div class="resume-item d-flex flex-column flex-md-row">
                  <div class="resume-content mr-auto">
                    <img src="research/icon/AIFinal.png" alt="PontTrust" align="middle" style="width: 70%;display:block;margin-left: auto;margin-right: auto;">
                    <br />
                    <strong><papertitle class=blue><a href="research/AI_CS181_project.pdf">Twitter Emotion Classification</a></papertitle></strong>
                    <br>Yiteng Xu*, <strong>Ziyao Zeng*</strong>, Jirui Shi*, Shaoxun Wu*, Peiyan Gu*</strong><br>
                    <span>Final Project of CS181 Artificial Intelligence, 2021 Fall, ShanghaiTech University</span><br>
                    <a href="https://github.com/Adonis-galaxy/homepage/blob/main/research/AI_CS181_project.pdf">code</a>
                    <font size="2.5"><li>We aim deal with twitter emotion classification by implementing Naive Bayes-based methods and DNN based methods with some adaptations like TF-IDF.</li>
                      <li>We provide throughout ablation studies, visualizations, and error mode analysis. We obtained comparable results in this task with existing methods.</li></font>
                   </div>
                </div>
                <br>

                <!-- Temporarily removed -->
                <hr>
                <div class="resume-item d-flex flex-column flex-md-row">
                  <div class="resume-content mr-auto">
                    <img src="research/icon/MLFinal.png" alt="PontTrust" align="middle" style="width: 70%;display:block;margin-left: auto;margin-right: auto;">
                    <br />
                    <strong><papertitle class=blue><a href="research/ML_CS282_project.pdf">Generalized DUQ: Generalized Deterministic Uncertainty Quantification</a></papertitle></strong>
                    <br>Zhitong Gao*, <strong>Ziyao Zeng*</strong><br>
                    <span>Final Project of CS282 Machine Learning, 2021 Spring, ShanghaiTech University</span><br>
                    <font size="2.5"><li>We propose Generalized DUQ to extend the original uncertainty estimation of only utilizing its nearest centroid to utilize the nearest k centroids.</li>
                    <li>Compared with DUQ, we achieve a better generlization on Two Moons, FashionMNIST VS MNIST/NotMNIST, and CIFAR-10 VS SVHN.</li></font>
                  </div>
                </div>
                <br>

              <h5>2020</h5>

                <hr>
                <div class="resume-item d-flex flex-column flex-md-row">
                  <div class="resume-content mr-auto">
                    <img src="research/icon/DLFinal.png" alt="PontTrust" align="middle" style="width: 70%;display:block;margin-left: auto;margin-right: auto;">
                    <br />
                    <strong><papertitle class=blue><a href="research/DL_CS280_project.pdf">Seek Common while Shelving Differences: A New Way for dealing with Noisy Labels</a></papertitle></strong>
                    <br>Zhitong Gao*, <strong>Ziyao Zeng*</strong><br>
                    <span>Final Project of CS280 Deep Learning, 2020 Fall, ShanghaiTech University</span><br>
                    <font size="2.5"><li>We propose SCSD, which combines both the benefits of “cross training” and “agreement” by introducing a tri-net framework, to deal with learning with noisy labels. </li>
                    <li>Extensive experimental results on corrupted data from benchmark datasets including MNIST, CIFAR-10, CIFAR-100 demonstrate that SCSD is superior to many state-of-the-art approaches.</li></font>
                  </div>
                </div>
                <br>

          </div>

  <!-- Enterprise -->
  <div class="tab-pane fade" id="enterprise" role="tabpanel" aria-labelledby="enterprise-tab">
    <h4 style="margin-top: 10px">My Path towards Enterprise</h4>
  <p>
    <li>Besides my CS major, I minor in <a href=https://sem.shanghaitech.edu.cn/sem_en/2017/0724/c3725a29609/page.htm>Innovation and Entrepreneurship</a> at <a href="https://www.shanghaitech.edu.cn/eng/">ShanghaiTech University</a>, as part of which I attended <a href="https://www.babson.edu/academics/babson-academy/student-programs/entrepreneurial-mindshift/">Entrepreneurial Mindshift Program</a> offered by <a href="https://www.babson.edu/">Babson College</a>for two weeks in March 2021</li>
    <p></p>
    <li>I believe the purpose of research is to address problems the world is facing and to benefit society. Historically, momentous technological developments have led to significant social advances. The reformation of the steam engine released laborers from certain types of toil, and the invention of the computer has likewise resulted in the declining economic role of labor in repetitive tasks. I believe the next evolution of productivity will be the automation and intellectualization brought by AI technology, which, I am optimistic, will further liberate humans’ creativity and wisdom. </li>
    <p></p>
    <li>Many AI technologies currently emerge from applications in real-world industrial problems. With my training in Entrepreneurship, I have the aptitude to scrutinize the inclination and pain points of the industrial field, so as to pilot hands-on research that generates tangible and influential outcomes. One day, with this grounding, I aspire to transform my research achievements into a startup and bring value to more people.</li>
    <p></p>
    <li>Towards this goal, being in this cosmopolitan city——Shanghai, I have been involved in various industrial exhibitions and entrepreneurship forums like <a href=https://www.worldaic.com.cn>WAIC</a>,<a href=http://www.visionchinashow.net>Vision China</a>, <a href=http://www.world-of-photonics-china.com.cn/zh-cn>Laser Photonics China</a>, and <a href=http://www.cme021.com>CME</a>, among others, so as to catch up with contemporary AI application in industry, and think about what contribution I could bring to this field.</li>
  </p>


  </div>
          <!-- Adventure -->
          <div class="tab-pane fade" id="adventure" role="tabpanel" aria-labelledby="adventure-tab">
            <h4 style="margin-top: 10px">My Adventure</h4>
            
                  <li>I am a big fan of adventure who is enthusiastic about cycling, hiking and mountain climbing.</li>
                  <li>"Being a scientist and an adventurer has a lot of similarities, they both want to achieve something that hasn't been achieved before."</li>
                  <li>In 2015, I have hiked across Lake District of England in 1 week.</li>
                  <img style="width:32%;max-width:100%" align="middle" src="images/lakedistrict/DSC_0015.JPG">
                  <img style="width:32%;max-width:100%" align="middle" src="images/lakedistrict/DSC_1167.JPG">
                  <img style="width:32%;max-width:100%" align="middle" src="images/lakedistrict/DSC_1442.JPG">
                  <li>In 2019, I have cycled cross Tibet for 28 days from <a href="https://sco.wikipedia.org/wiki/Chengdu">Chengdu</a> to <a href="https://de.wikipedia.org/wiki/Lhasa">Lhasa</a> for 2135 km.</li>
                  <p></p>
                  <img style="width:97%;max-width:100%" align="middle" src="images/qixingluxian.jpg">
                  <img style="width:32%;max-width:100%" align="middle" src="images/zhongdian.jpg">
                  <img style="width:32%;max-width:100%" align="middle" src="images/mila.jpg">
                  <img style="width:32%;max-width:100%" align="middle" src="images/chuanzang3.jpg">
                  <img style="width:24%;max-width:100%" align="middle" src="images/bingchuan.jpg">
                  <img style="width:24%;max-width:100%" align="middle" src="images/3838.jpg">
                  <img style="width:24%;max-width:100%" align="middle" src="images/chuanzang1.jpg">
                  <img style="width:24%;max-width:100%" align="middle" src="images/dongda.jpg">
                  <p></p>
                  <li>In 2022, I have cycled cross Tibet and Xinjiang for 1 month from <a href=https://en.wikipedia.org/wiki/%C3%9Cr%C3%BCmqi">Ürümqi</a> to <a href="https://de.wikipedia.org/wiki/Lhasa">Lhasa</a> for 5000 km, with about 2000 km cycling at an average altitude of 4500 m.</li>
                  <p></p>
                  <img style="width:32%;max-width:100%" align="middle" src="images/xinzangxian/plan.jpg">
                  <img style="width:32%;max-width:100%" align="middle" src="images/xinzangxian/qidian.jpg">
                  <img style="width:32%;max-width:100%" align="middle" src="images/xinzangxian/binhe.jpg">
                  <img style="width:32%;max-width:100%" align="middle" src="images/xinzangxian/shanding.jpg">
                  <img style="width:32%;max-width:100%" align="middle" src="images/xinzangxian/swim.jpg">
                  <img style="width:32%;max-width:100%" align="middle" src="images/xinzangxian/zhufeng.jpg">

                  <img style="width:24%;max-width:100%" align="middle" src="images/xinzangxian/zhongdian.jpg">
                  <img style="width:24%;max-width:100%" align="middle" src="images/xinzangxian/daban.jpg">
                  <img style="width:24%;max-width:100%" align="middle" src="images/xinzangxian/gangrenboqi.jpg">                                
                  <img style="width:24%;max-width:100%" align="middle" src="images/xinzangxian/zhuanshan.jpg">
                  <img style="width:24%;max-width:100%" align="middle" src="images/xinzangxian/zipai.jpg">
                  
                  <img style="width:24%;max-width:100%" align="middle" src="images/xinzangxian/lasa.jpg">
                  <img style="width:24%;max-width:100%" align="middle" src="images/xinzangxian/zhengshuheying.jpg">
                  <img style="width:24%;max-width:100%" align="middle" src="images/xinzangxian/zhengshu.jpg">
                  <p></p>
                  <li>In 2023, I hiked in <a href="https://www.chinadiscovery.com/yunnan/yubeng-village.html">Yubeng Village</a> for 5 days, across an altitude between 3000 m to 4300 m.</li>
                  <img style="width:24%;max-width:100%" align="middle" src="images/yubeng/1.jpg">
                  <img style="width:24%;max-width:100%" align="middle" src="images/yubeng/2.jpg">
                  <img style="width:24%;max-width:100%" align="middle" src="images/yubeng/3.jpg">
                  <img style="width:24%;max-width:100%" align="middle" src="images/yubeng/4.jpg">
                  <img style="width:24%;max-width:100%" align="middle" src="images/yubeng/5.jpg">
                  <img style="width:24%;max-width:100%" align="middle" src="images/yubeng/6.jpg">
                  <p></p>
                  <p></p>
                  <li>In 2023, I hiked in <a href="https://en.wikipedia.org/wiki/Tiger_Leaping_Gorge">Tiger Leaping Gorge</a> High Road for 2 days.
                  <img style="width:24%;max-width:100%" align="middle" src="images/TLG/tlg1.jpg">
                  <img style="width:24%;max-width:100%" align="middle" src="images/TLG/tlg2.jpg">
                  <img style="width:24%;max-width:100%" align="middle" src="images/TLG/tlg3.jpg">
                  <p></p>
                  <p></p>

                  <p></p>
                  <li>My other photos regarding adventures.</li>
                  <img style="width:24%;max-width:100%" align="middle" src="images/zengzy.jpg">
                  <img style="width:24%;max-width:100%" align="middle" src="images/cover.jpg">
                  <img style="width:24%;max-width:100%" align="middle" src="images/dongshan.jpg">
                  <img style="width:24%;max-width:100%" align="middle" src="images/changsha.jpg">
                  <img style="width:24%;max-width:100%" align="middle" src="images/meili.jpg">
                  <p></p>
                  


          </div>
          <!-- Others about me -->
      <div class="tab-pane fade" id="others" role="tabpanel" aria-labelledby="others-tab">
            <h4 style="margin-top: 10px">Other things about myself</h4>
            <p>
              <li>I'm a disciplined body builder.</li>
              <li>"Discipline is doing what you hate to do, but do it as you love it."</li>
              <li>I always enforce such discipline not only into my body-building but my work and life.</li>
              <img style="width:24%;max-width:100%" align="middle" src="images/body.jpg">
            </p>
            <hr /> <!-- 画一条横线-->
            <p> 
              <li>Besides, I'm an amateur sci-fi novelist.</li>
              <li>I continuously think about the formation of technology and society in the future, especially their influence on human beings. With the development of AI technology, the field of careers will be totally shaped within a few decades, which is related to everyone's benefit. It's my duty to foresee this transformation and try my best to turn it into a good way. As a sci-fi lover, my favoriate novelist is <a href="https://en.wikipedia.org/wiki/Isaac_Asimov">Isaac Asimov</a> with his great work <a href="https://en.wikipedia.org/wiki/Foundation_(Asimov_novel)">Foundation</a></li>
              <li><a href="https://github.com/Adonis-galaxy/Adonis-Sci-Fi-Novel">Link to my sci-fi works</a>
            </p>
            <hr /> <!-- 画一条横线-->
            <p> 
              <li>I'm an amateur Unity game developer, previous supervised by <a href="http://www.briancox.be/?page=home">Brain Cox</a>, screenshots of my previous works have been shown below.</li>
              <li>Snow Ranger</li>
              <img style="max-width:50%" align="middle" src="images/Unity_HW2.png">
              <li><a href="https://www.bilibili.com/video/BV1WS4y1Z7rz?p=1&share_medium=ipad&share_plat=ios&share_source=QQ&share_tag=s_i&timestamp=1641184133&unique_k=GHTLVUs">Darkside</a></li>
              <img style="max-width:33%" align="middle" src="images/main_menu.jpg">
              <img style="max-width:33%" align="middle" src="images/night_vision.jpg">
              <img style="max-width:33%" align="middle" src="images/puzzle.jpg">

            </p>
            <hr /> <!-- 画一条横线-->
            <p> 
              <li>I'm also an amateur composer, conducter, pianist, trombone player.</li>
              <li>I have been playing Tarot since 2014, familiar with Thoth and Shadowflower, dedicated to combining Tarot with modern psychology to serve as a tool for consciousness.</li>
            </p>
            <hr /> <!-- 画一条横线-->
            <p> 
              <li>I'm excited about all kinds of voluntary especially those related to environment protection.</li>
              <li>I believe it's our instinctive duty to preserve the integrity of the earth (at least until we could immigrate to other planets).</li>
              <li>Currently, I'm volunteering at <a href="http://www.wwfchina.org/">WWF-China</a> and <a href="http://www.greenpeace.org.cn/">Greenpeace</a></li>
            </p>
        </div>

          <!-- 结尾功能组件 -->
        </div>
      </div>
    <br>
    <br>

    <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>

  </body>

</html>
