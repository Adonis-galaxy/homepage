
<!DOCTYPE html>
<html lang="en">

  <head>
    <meta charset="utf-8">
    <link rel="icon" type="image/png" href="images/Yale.png">
  	<meta charset="utf-8">

    <style type="text/css">
      a:link {
        color:rgb(0, 47, 164);
        text-decoration:none;
      }
      a:visited {
        color:rgb(0, 47, 164);
        text-decoration:none;
      }
      a:hover {
        color:rgb(0, 47, 164);
        text-decoration:none;
      }
      a:active {
        color:rgb(0, 47, 164);
        text-decoration:none;
      }

      .highlight-entry {
  background-color: #fffdce; /* Light orange */
  padding: 10px;
  border-radius: 5px;
}



    </style>
    <title>Ziyao Zeng</title>

    <!-- Bootstrap core CSS -->
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">

    <!-- Custom styles for this template -->
    <link href="css/style.css" rel="stylesheet">
  </head>

  <body>

    <div class="container col-xs-12 col-sm-8 col-md-8 col-lg-8 col-md-offset-2 col-lg-offset-2 col-sm-offset-2 p-0">

        <h1 id = "myName" align ="center">Ziyao (Adonis)
            <span class="text-primary">Zeng</span> 曾子尧
        </h1>
<!-- Bottons -->
        <ul class="nav nav-tabs" id="myTab" role="tablist">
        <li class="nav-item">
          <a class="nav-link active" id="home-tab" data-toggle="tab" href="#home" role="tab" aria-controls="home" aria-selected="true">Home</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" id="publication-tab" data-toggle="tab" href="#publication" role="tab" aria-controls="publication" aria-selected="false">Research</a>
        </li>
        <!-- <li class="nav-item">
          <a class="nav-link" id="enterprise-tab" data-toggle="tab" href="#enterprise" role="tab" aria-controls="enterprise" aria-selected="false">Enterprise</a>
        </li> -->
        <li class="nav-item">
          <a class="nav-link" id="adventure-tab" data-toggle="tab" href="#adventure" role="tab" aria-controls="adventure" aria-selected="false">Adventure</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" id="others-tab" data-toggle="tab" href="#others" role="tab" aria-controls="others" aria-selected="false">Others</a>
        </li>
      </ul>
      <div class="tab-content" id="nav-tabContent">
<!-- Main Body -->
  <!-- Introduction -->
    <div class="tab-pane fade show active" id="home" role="tabpanel" aria-labelledby="home-tab">


        <table>
          <tr>
            <td width = "62%">
              <p align>
                <!-- <h4 style="margin-top: 10px">Starting AI-Scientist with Enterprise</h4>    -->

                I'm a second-year Ph.D. student in Computer Science (2023 - [Expected] 2028) at <a href="https://www.yale.edu/">Yale University</a>, supervised by <a href="https://www.cs.yale.edu/homes/wong-alex/">Prof. Alex Wong</a>. Previous to that, I obtained my B.Eng. in Computer Science (2019 - 2023) at <a href="https://www.shanghaitech.edu.cn/eng/">ShanghaiTech University</a>, minor in <a href="https://sem.shanghaitech.edu.cn/sem_en/2017/0724/c3725a29609/page.htm">Innovation and Entrepreneurship</a>.

                <p></p>

                I am an incoming PhD Research Intern at <a href="https://www.nvidia.com/en-us/research/">Nvidia Research</a>, mentoring by <a href="https://oraziogallo.github.io/">Orazio Gallo</a>. Previously, I interened with <a href="https://www.cis.upenn.edu/~jshi/">Prof. Jianbo Shi</a> at <a href="https://www.grasp.upenn.edu/">UPenn GRASP Lab</a>, with <a href="https://xmhe.bitbucket.io/">Prof. Xuming He</a> at <a href="http://plus.sist.shanghaitech.edu.cn/">ShanghaiTech PLUS Group</a>.

                <p></p>
                I conduct research on Computer Vision, Machine Learning, and Robotics. I mainly focus on <strong>Multimodal Embodied AI</strong> inspired by human learning. Currently, my research mainly lies in <strong>Vision-Language Models for 3D Vision (Perception and Reconstruction)</strong>.
                <p></p>
                <!-- <a href="Long CV-Ziyao Zeng.pdf">Long CV</a> / -->
                <a href="https://scholar.google.com/citations?hl=en&user=FYL2DYEAAAAJ">Google Scholar</a> /
                <a href="https://github.com/Adonis-galaxy">GitHub</a> /
                <a href="http://vision.cs.yale.edu/members/ziyao-zeng.html">Yale Vision Lab</a>
                <p></p>
                <!-- Reviewer: CVPR 2022, ICCV 2023, ACM MM 2023, ICASSP 2024, ECCV 2024, NeurIPS 2024, ICPR 2024, ACCV 2024, TCSVT (2024), ICLR 2025, ICASSP 2025, AISTATS 2025, CVPR 2025 -->
                Reviewer: CVPR, ICCV, ECCV, NeurIPS, ICLR, ICML
                <p></p>
                Email: ziyao.zeng (at) yale.edu
                <p></p>
                <!-- <strong>Feel free to reach out for collaborations, entrepreneurship, questions, or to connect on WeChat~</strong> -->


            </td>
            <td width = "1%">
            </td>
            <td width = "30%">
              <img src="images/cover_full_body.jpg" alt="PontTrust" align="middle" style="width: 100%;">
            </td>
        </tr>

        </table>
        <br>
        <p id = "linkToJiarui">Website format from <a href="https://www.cs.utexas.edu/~zhouxy/#publication">Xingyi Zhou</a>.</p>
        <p align="center"> Last updated April 2025</p>  <!-- Time -->
        </div>

        <!-- Research -->
        <div class="tab-pane fade" id="publication" role="tabpanel" aria-labelledby="publication-tab">
              <h4 style="margin-top: 10px">Research Overview</h4>
              <p>
                <li>Since the age of 13, deeply touched by <i><a href="https://en.wikipedia.org/wiki/Foundation_(Asimov_novel)">Foundation</a></i> by <a href="https://en.wikipedia.org/wiki/Isaac_Asimov">Isaac Asimov</a>, my dream has been to create an AI who can think like humans (just like the dream of <a href="https://people.idsia.ch//~juergen/">Prof. Jürgen Schmidhuber</a>). When humans perceive the surrounding environment, we see (2D vision), touch (tactile), wander (3D vision), and hear (audio) simultaneously to understand (neural signal) and interpret (language). Therefore, I conduct research on <strong>Multimodal Embodied AI</strong>. My research vision is to empower embodied AI with multimodal sensing, and can leverage pre-trained multimodal representations, to interact with the physical world as humans do.</li>
                <p></p>
                 <li>Specifically, I conduct research on <strong>Language for 3D Vision (Perception and Reconstruction)</strong>. Given one language description, one can easily imagine what this scene could look like, so language could easily be interpreted as a condition to generate and manipulate 3D scenes in a controllable manner. On the other hand, language description could serve as a prior that is specific for a given scene to enhance 3D reconstruction by resolving scale ambiguity. Language description itself can also be used to tell ordinal relationships between different objects, so that to infer their depth. The use of language descriptions, which is invariant to nuisance variability (e.g., illumination, occlusion, viewpoints in images), provides extra robust features to assist models’ generalization. Practically, language is arguably cheaper to obtain than range measurements (e.g., from lidar, radar).</li>
                <p></p>

              </p>

              <!-- Publications -->
              <h4>Publications</h4>
              (* indicates equal contributions; Selected publications of "Language for 3D Vision" are highlighted.)
              <h5>2025</h5>
              <hr>
              <div class="resume-item d-flex flex-column flex-md-row">
                <div class="resume-content mr-auto">
                  <img src="research/icon/ProtoDepth.png" alt="PontTrust" align="middle" style="width: 70%;display:block;margin-left: auto;margin-right: auto;">
                  <br />
                  <strong><papertitle class=blue><a href="https://arxiv.org/abs/2503.12745">ProtoDepth: Unsupervised Continual Depth Completion with Prototypes</a></papertitle></strong>
                  <br>Patrick Rim, Hyoungseob Park, S. Gangopadhyay, <strong>Ziyao Zeng</strong>, Younjoon Chung, Alex Wong<br>
                  <span>CVPR 2025</span>
                </div>
              </div>
              <br>

              <hr>
              <div class="resume-item d-flex flex-column flex-md-row">
                <div class="resume-content mr-auto">
                  <img src="research/icon/HOMER.png" alt="PontTrust" align="middle" style="width: 70%;display:block;margin-left: auto;margin-right: auto;">
                  <br />
                  <strong><papertitle class=blue><a href="https://arxiv.org/abs/2501.17636">HOMER: Homography-Based Efficient Multi-view 3D Object Removal</a></papertitle></strong>
                  <br>Jingcheng Ni*, Weiguang Zhao*, Daniel Wang, <strong>Ziyao Zeng</strong>, Chenyu You, Alex Wong, Kaizhu Huang<br>
                  <span>arXiv technical report, 2025</span>
                </div>
              </div>
              <br>


              <h5>2024</h5>
              <hr>
                <div class="resume-content mr-auto">
                  <img src="research/icon/PriorDiffusion.png" alt="PontTrust" align="middle" style="width: 70%;display:block;margin-left: auto;margin-right: auto;">
                  <br />
                  <div class="highlight-entry">
                    <strong><papertitle class=blue><a href="https://www.arxiv.org/abs/2411.16750">PriorDiffusion: Leverage Language Prior in Diffusion Models for Monocular Depth Estimation</a></papertitle></strong>
                    <br><strong>Ziyao Zeng</strong>, Jingcheng Ni, Daniel Wang, Patrick Rim, Younjoon Chung, Fengyu Yang, Byung-Woo Hong, Alex Wong<br>
                    <span>arXiv technical report, 2024</span><br>
                  </div>
                </div>
              <br>


              <hr>
                <div class="resume-content mr-auto">
                  <img src="research/icon/RSA.png" alt="PontTrust" align="middle" style="width: 70%;display:block;margin-left: auto;margin-right: auto;">
                  <br />
                  <div class="highlight-entry">
                    <strong><papertitle class=blue><a href="http://arxiv.org/abs/2410.02924">RSA: Resolving Scale Ambiguities in Monocular Depth Estimators through Language Descriptions</a></papertitle></strong>
                    <br><strong>Ziyao Zeng</strong>, Yangchao Wu, Hyoungseob Park, Daniel Wang, Fengyu Yang, Stefano Soatto, Dong Lao, Byung-Woo Hong, Alex Wong<br>
                    <span>NeurIPS 2024</span><br>
                  </div>
                </div>
              <br>



              <hr>
              <div class="resume-item d-flex flex-column flex-md-row">
                <div class="resume-content mr-auto">
                  <img src="research/icon/NeuroBind.png" alt="PontTrust" align="middle" style="width: 70%;display:block;margin-left: auto;margin-right: auto;">
                  <br />
                  <strong><papertitle class=blue><a href="https://arxiv.org/abs/2407.14020">NeuroBind: Towards Unified Multimodal Representations for Neural Signals</a></papertitle></strong>
                  <br>Fengyu Yang*, Chao Feng*, Daniel Wang*, Tianye Wang, <strong>Ziyao Zeng</strong>, Zhiyang Xu, Hyoungseob Park, Pengliang Ji, Hanbin Zhao, Yuanning Li, Alex Wong<br>
                  <span>arXiv technical report, 2024</span><br>
                </div>
              </div>
              <br>


              <h5>2023</h5>
              <hr>
                <div class="resume-content mr-auto">
                  <img src="research/icon/WorDepth.png" alt="PontTrust" align="middle" style="width: 70%;display:block;margin-left: auto;margin-right: auto;">
                  <br />
                  <div class="highlight-entry">
                    <strong><papertitle class=blue><a href="https://arxiv.org/abs/2404.03635">WorDepth: Variational Language Prior for Monocular Depth Estimation</a></papertitle></strong>
                    <br><strong>Ziyao Zeng</strong>, Daniel Wang, Fengyu Yang, Hyoungseob Park, Yangchao Wu, Stefano Soatto, Byung-Woo Hong, Dong Lao, Alex Wong<br>
                    <span>CVPR 2024</span><br>
                    <a href="https://github.com/Adonis-galaxy/WorDepth">code</a>
                  </div>
                </div>
              </div>
              <br>


              <hr>
              <div class="resume-item d-flex flex-column flex-md-row">
                <div class="resume-content mr-auto">
                  <img src="research/icon/UniTouch.png" alt="PontTrust" align="middle" style="width: 70%;display:block;margin-left: auto;margin-right: auto;">
                  <br />
                  <strong><papertitle class=blue><a href="https://arxiv.org/abs/2401.18084">Binding Touch to Everything: Learning Unified Multimodal Tactile Representations</a></papertitle></strong>
                  <br>Fengyu Yang*, Chao Feng*, Ziyang Chen*, Hyoungseob Park, Daniel Wang, Yiming Dou, <strong>Ziyao Zeng</strong>, Xien Chen, Rit Gangopadhyay, Andrew Owens, Alex Wong<br>
                  <span>CVPR 2024</span><br>
                  <a href="https://cfeng16.github.io/UniTouch/">project page</a>, <a href="https://github.com/cfeng16/UniTouch">code</a>
                </div>
              </div>
              <br>


              <h5>2022</h5>
              <hr>
                <div class="resume-content mr-auto">
                  <img src="research/icon/PointCLIPv2.png" alt="PontTrust" align="middle" style="width: 70%;display:block;margin-left: auto;margin-right: auto;">
                  <br />
                  <div class="highlight-entry">
                    <strong><papertitle class=blue><a href="https://arxiv.org/abs/2211.11682">PointCLIP V2: Prompting CLIP and GPT for Powerful 3D Open-world Learning</a></papertitle></strong>
                    <br>Xiangyang Zhu*, Renrui Zhang*, Bowei He, Ziyu Guo, <strong>Ziyao Zeng</strong>, Zipeng Qin, Shanghang Zhang, Peng Gao<br>
                    <span>ICCV 2023</span><br>
                    <a href="https://github.com/yangyangyang127/PointCLIP_V2">code</a>
                  </div>
                </div>
              </div>
              <br>


              <hr>
              <div class="resume-item d-flex flex-column flex-md-row">
                <div class="resume-content mr-auto">
                  <img src="research/icon/iQuery.png" alt="PontTrust" align="middle" style="width: 70%;display:block;margin-left: auto;margin-right: auto;">
                  <br />
                  <strong><papertitle class=blue><a href="https://arxiv.org/abs/2212.03814">iQuery: Instruments as Queries for Audio-Visual Sound Separation</a></papertitle></strong>
                  <br>Jiaben Chen, Renrui Zhang, Dongze Lian, Jiaqi Yang, <strong>Ziyao Zeng</strong>, Jianbo Shi<br>
                  <span>CVPR 2023</span><br>
                  <a href="https://github.com/JiabenChen/iQuery">code</a>
                </div>              </div>
              <br>




              <hr>
                <div class="resume-content mr-auto">
                  <img src="research/icon/depthclip.png" alt="PontTrust" align="middle" style="width: 70%;display:block;margin-left: auto;margin-right: auto;">
                  <br />
                  <div class="highlight-entry">
                    <strong><papertitle class=blue><a href="https://arxiv.org/abs/2207.01077">Can Language Understand Depth?</a></papertitle></strong>
                    <br>Renrui Zhang*, <strong>Ziyao Zeng*</strong>, Ziyu Guo, Yafeng Li<br>
                    <span>ACM Multimedia 2022, accepted as Brave New Idea (Accepte Rate<=12.5%)</span><br>
                    <a href="https://github.com/Adonis-galaxy/DepthCLIP">code</a>
                  </div>
                </div>
              <br>



              <h5>2021</h5>
                <hr>
                <div class="resume-item d-flex flex-column flex-md-row">
                  <div class="resume-content mr-auto">
                    <img src="research/icon/dspoint.png" alt="PontTrust" align="middle" style="width: 70%;display:block;margin-left: auto;margin-right: auto;">
                    <br />
                    <strong><papertitle class=blue><a href="https://arxiv.org/abs/2111.10332">DSPoint: Dual-scale Point Cloud Recognition with High-frequency Fusion</a></papertitle></strong>
                    <br>Renrui Zhang*, <strong>Ziyao Zeng*</strong>, Ziyu Guo, Xinben Gao, Kexue Fu, Jianbo Shi <br>
                    <span>SMC 2023</span><br>
                    <a href="https://github.com/Adonis-galaxy/DSPoint">code</a>
                  </div>
                </div>
                <br>


                <hr>
                <div class="resume-item d-flex flex-column flex-md-row">
                  <div class="resume-content mr-auto">
                    <img src="research/icon/vtclip.png" alt="PontTrust" align="middle" style="width: 70%;display:block;margin-left: auto;margin-right: auto;">
                    <br />
                    <strong><papertitle class=blue><a href="https://arxiv.org/abs/2112.02399">VT-CLIP: Enhancing Vision-Language Models with Visual-guided Texts</a></papertitle></strong>
                    <br>Longtian Qiu, Renrui Zhang, Ziyu Guo, <strong>Ziyao Zeng</strong>, Yafeng Li, Guangnan Zhang<br>
                    <span>arXiv technical report, 2021</span><br>
                  </div>
                </div>
                <br>


                <hr>
                <div class="resume-item d-flex flex-column flex-md-row">
                  <div class="resume-content mr-auto">
                    <img src="research/icon/AIFinal.png" alt="PontTrust" align="middle" style="width: 70%;display:block;margin-left: auto;margin-right: auto;">
                    <br />
                    <strong><papertitle class=blue><a href="research/AI_CS181_project.pdf">Twitter Emotion Classification</a></papertitle></strong>
                    <br>Yiteng Xu*, <strong>Ziyao Zeng*</strong>, Jirui Shi*, Shaoxun Wu*, Peiyan Gu*<br>
                    <span>Final Project of CS181 Artificial Intelligence, 2021 Fall, ShanghaiTech University</span><br>
                    <a href="https://github.com/Adonis-galaxy/Twitter-Emotion-Classification">code</a>
                   </div>
                </div>
                <br>

                <hr>
                <div class="resume-item d-flex flex-column flex-md-row">
                  <div class="resume-content mr-auto">
                    <img src="research/icon/MLFinal.png" alt="PontTrust" align="middle" style="width: 70%;display:block;margin-left: auto;margin-right: auto;">
                    <br />
                    <strong><papertitle class=blue><a href="research/ML_CS282_project.pdf">Generalized DUQ: Generalized Deterministic Uncertainty Quantification</a></papertitle></strong>
                    <br>Zhitong Gao*, <strong>Ziyao Zeng*</strong><br>
                    <span>Final Project of CS282 Machine Learning, 2021 Spring, ShanghaiTech University</span><br>
                  </div>
                </div>
                <br>

              <h5>2020</h5>

                <hr>
                <div class="resume-item d-flex flex-column flex-md-row">
                  <div class="resume-content mr-auto">
                    <img src="research/icon/DLFinal.png" alt="PontTrust" align="middle" style="width: 70%;display:block;margin-left: auto;margin-right: auto;">
                    <br />
                    <strong><papertitle class=blue><a href="research/DL_CS280_project.pdf">Seek Common while Shelving Differences: A New Way for dealing with Noisy Labels</a></papertitle></strong>
                    <br>Zhitong Gao*, <strong>Ziyao Zeng*</strong><br>
                    <span>Final Project of CS280 Deep Learning, 2020 Fall, ShanghaiTech University</span><br>
                  </div>
                </div>
                <br>

          </div>

          <!-- Adventure -->
          <div class="tab-pane fade" id="adventure" role="tabpanel" aria-labelledby="adventure-tab">
            <h4 style="margin-top: 10px">My Adventure</h4>

                  <li>I am a big fan of adventure who is enthusiastic about cycling, hiking and mountain climbing.</li>
                  <li>"Being a scientist and an adventurer has a lot of similarities, they both want to achieve something that hasn't been achieved before."</li>
                  <li>In 2015, I have hiked across Lake District of England in 1 week.</li>
                  <img style="width:32%;max-width:100%" align="middle" src="images/lakedistrict/DSC_0015.JPG">
                  <img style="width:32%;max-width:100%" align="middle" src="images/lakedistrict/DSC_1167.JPG">
                  <img style="width:32%;max-width:100%" align="middle" src="images/lakedistrict/DSC_1442.JPG">
                  <li>In 2019, I have cycled cross Tibet for 28 days from <a href="https://sco.wikipedia.org/wiki/Chengdu">Chengdu</a> to <a href="https://de.wikipedia.org/wiki/Lhasa">Lhasa</a> for 2135 km.</li>
                  <p></p>
                  <img style="width:97%;max-width:100%" align="middle" src="images/318/qixingluxian.jpg">
                  <img style="width:32%;max-width:100%" align="middle" src="images/318/zhongdian.jpg">
                  <img style="width:32%;max-width:100%" align="middle" src="images/318/mila.jpg">
                  <img style="width:32%;max-width:100%" align="middle" src="images/318/chuanzang3.jpg">
                  <img style="width:24%;max-width:100%" align="middle" src="images/318/bingchuan.jpg">
                  <img style="width:24%;max-width:100%" align="middle" src="images/318/3838.jpg">
                  <img style="width:24%;max-width:100%" align="middle" src="images/318/chuanzang1.jpg">
                  <img style="width:24%;max-width:100%" align="middle" src="images/318/dongda.jpg">
                  <p></p>
                  <li>In 2022, I have cycled cross Tibet and Xinjiang for 1 month from <a href=https://en.wikipedia.org/wiki/%C3%9Cr%C3%BCmqi">Ürümqi</a> to <a href="https://de.wikipedia.org/wiki/Lhasa">Lhasa</a> for 5000 km, with about 2000 km cycling at an average altitude of 4500 m.</li>
                  <p></p>
                  <img style="width:32%;max-width:100%" align="middle" src="images/xinzangxian/plan.jpg">
                  <img style="width:32%;max-width:100%" align="middle" src="images/xinzangxian/qidian.jpg">
                  <img style="width:32%;max-width:100%" align="middle" src="images/xinzangxian/binhe.jpg">
                  <img style="width:32%;max-width:100%" align="middle" src="images/xinzangxian/shanding.jpg">
                  <img style="width:32%;max-width:100%" align="middle" src="images/xinzangxian/swim.jpg">
                  <img style="width:32%;max-width:100%" align="middle" src="images/xinzangxian/zhufeng.jpg">

                  <img style="width:24%;max-width:100%" align="middle" src="images/xinzangxian/zhongdian.jpg">
                  <img style="width:24%;max-width:100%" align="middle" src="images/xinzangxian/daban.jpg">
                  <img style="width:24%;max-width:100%" align="middle" src="images/xinzangxian/gangrenboqi.jpg">
                  <img style="width:24%;max-width:100%" align="middle" src="images/xinzangxian/zhuanshan.jpg">
                  <img style="width:24%;max-width:100%" align="middle" src="images/xinzangxian/zipai.jpg">

                  <img style="width:24%;max-width:100%" align="middle" src="images/xinzangxian/lasa.jpg">
                  <img style="width:24%;max-width:100%" align="middle" src="images/xinzangxian/zhengshuheying.jpg">
                  <img style="width:24%;max-width:100%" align="middle" src="images/xinzangxian/zhengshu.jpg">
                  <p></p>
                  <li>In 2023, I hiked in <a href="https://www.chinadiscovery.com/yunnan/yubeng-village.html">Yubeng Village</a> for 5 days, across an altitude between 3000 m to 4300 m.</li>
                  <img style="width:32%;max-width:100%" align="middle" src="images/yubeng/1.jpg">
                  <img style="width:32%;max-width:100%" align="middle" src="images/yubeng/2.jpg">
                  <img style="width:32%;max-width:100%" align="middle" src="images/yubeng/3.jpg">
                  <img style="width:32%;max-width:100%" align="middle" src="images/yubeng/4.jpg">
                  <img style="width:32%;max-width:100%" align="middle" src="images/yubeng/5.jpg">
                  <img style="width:32%;max-width:100%" align="middle" src="images/yubeng/6.jpg">
                  <p></p>
                  My hiking video in Ice Lake, 3700 m altitude <a href="https://youtu.be/_sd_18dW6IA">Link</a>
                  <p></p>
                  My hiking video in God Lake, 4300 m altitude: <a href="https://youtube.com/shorts/tSl27ym758s">Link</a>
                  <p></p>
                  <li>In 2023, I hiked in <a href="https://en.wikipedia.org/wiki/Tiger_Leaping_Gorge">Tiger Leaping Gorge</a> High Road for 2 days.</li>
                  <img style="width:32%;max-width:100%" align="middle" src="images/TLG/tlg1.jpg">
                  <img style="width:32%;max-width:100%" align="middle" src="images/TLG/tlg2.jpg">
                  <img style="width:32%;max-width:100%" align="middle" src="images/TLG/tlg3.jpg">
                  <p></p>
                  <p></p>

                  <li>In 2023, I cycle around <a href="https://en.wikipedia.org/wiki/Qinghai_Lake">Qinghai Lake</a> more than 350 km for 4 days .</li>
                  <img style="width:24%;max-width:100%" align="middle" src="images/qinghai/IMG_0171.JPG">
                  <img style="width:24%;max-width:100%" align="middle" src="images/qinghai/IMG_0172.JPG">
                  <img style="width:24%;max-width:100%" align="middle" src="images/qinghai/IMG_0173.JPG">
                  <img style="width:24%;max-width:100%" align="middle" src="images/qinghai/IMG_0174.JPG">

                  <li>In 2024, I got my diving certificate in the Red Sea.</li>
                  <img style="width:32%;max-width:100%" align="middle" src="images/diving/IMG_9411.JPG">
                  <img style="width:32%;max-width:100%" align="middle" src="images/diving/IMG_9412.JPG">
                  <img style="width:32%;max-width:100%" align="middle" src="images/diving/IMG_9414.JPG">
                  <p></p>

                  <li>In 2024, sucessfully climbed to the top of Mount Yuzhu, 6178 meters in altitude.</li>
                  <img style="width:24%;max-width:100%" align="middle" src="images/yuzhu/yuzhu_1.png">
                  <img style="width:24%;max-width:100%" align="middle" src="images/yuzhu/yuzhu_2.png">
                  <img style="width:24%;max-width:100%" align="middle" src="images/yuzhu/yuzhu_3.png">
                  <img style="width:24%;max-width:100%" align="middle" src="images/yuzhu/yuzhu_4.png">
                  <p></p>

                  <p></p>
                  <li>My other photos regarding adventures.</li>
                  <img style="width:24%;max-width:100%" align="middle" src="images/others/zengzy.jpg">
                  <img style="width:24%;max-width:100%" align="middle" src="images/others/cover.jpg">
                  <img style="width:24%;max-width:100%" align="middle" src="images/others/dongshan.jpg">
                  <img style="width:24%;max-width:100%" align="middle" src="images/others/changsha.jpg">
                  <img style="width:24%;max-width:100%" align="middle" src="images/others/meili.jpg">
                  <img style="width:24%;max-width:100%" align="middle" src="images/others/Paris.JPG">
                  <img style="width:24%;max-width:100%" align="middle" src="images/others/Karak.JPG">
                  <img style="width:24%;max-width:100%" align="middle" src="images/others/Maroc.JPG">
                  <img style="width:24%;max-width:100%" align="middle" src="images/others/ski.JPG">
                  <img style="width:24%;max-width:100%" align="middle" src="images/others/Egypt.JPG">
                  <img style="width:24%;max-width:100%" align="middle" src="images/others/ballon.JPG">
                  <p></p>



          </div>
          <!-- Others about me -->
      <div class="tab-pane fade" id="others" role="tabpanel" aria-labelledby="others-tab">
            <h4 style="margin-top: 10px">Other things about myself</h4>
            <p>
              <li>I'm an amateur Unity game developer, previous supervised by <a href="http://www.briancox.be/?page=home">Brain Cox</a>, screenshots of my previous works have been shown below.</li>
              <li>Snow Ranger</li>
              <img style="max-width:50%" align="middle" src="images/Unity/Unity_HW2.png">
              <li><a href="https://www.bilibili.com/video/BV1WS4y1Z7rz?p=1&share_medium=ipad&share_plat=ios&share_source=QQ&share_tag=s_i&timestamp=1641184133&unique_k=GHTLVUs">Darkside</a></li>
              <img style="max-width:33%" align="middle" src="images/Unity/main_menu.jpg">
              <img style="max-width:33%" align="middle" src="images/Unity/night_vision.jpg">
              <img style="max-width:33%" align="middle" src="images/Unity/puzzle.jpg">

            </p>
            <hr />
            <p>
              <li>I'm also an amateur composer, conducter, pianist, trombone player, guitar player, and Chinese folk signer.</li>
              <li>I have been playing Tarot since 2014, familiar with Thoth and Flower Shadow, dedicated to combining Tarot with modern psychology to serve as a tool for consciousness.</li>
            </p>
            <hr /> <!-- A line-->
            <p>
              <li>I'm excited about all kinds of voluntary especially those related to environment protection.</li>
              <li>I believe it's our instinctive duty to preserve the integrity of the earth (at least until we could immigrate to other planets).</li>
              <li>Currently, I'm volunteering at <a href="http://www.wwfchina.org/">WWF-China</a> and <a href="http://www.greenpeace.org.cn/">Greenpeace</a></li>
            </p>
        </div>

          <!-- Ending -->
        </div>
      </div>
    <br>
    <br>

    <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>
<!-- <script type="text/javascript" id="clstr_globe" src="//clustrmaps.com/globe.js?d=yuoBMXDrKL-QBzjeq858G6uTHIVF7jwRwEfLziX7UYU"></script> -->
<script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=080808&w=300&t=tt&d=yuoBMXDrKL-QBzjeq858G6uTHIVF7jwRwEfLziX7UYU&co=ffffff&cmo=3acc3a&cmn=ff5353&ct=808080'></script>

</body>

</html>
