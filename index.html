<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Ziyao Zeng</title>
  
  <meta name="author" content="Ziyao Zeng">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/headshot.JPG">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">

              <p style="text-align:center"><b><font size="6"><name>
              Ziyao Zeng
              </name></font></b></p>

              <font color="red" size="4"><heading><b>Who am I:</b></heading></font>
              <li>Sophomore of ShanghaiTech University majoring in Computer Science</li>
              <li>Innovative AI-Scientist with Enterprise</li>
              <li>Enthusiastic Adventurer</li>
                
              <li> 
                | <a href="https://www.linkedin.com/in/ziyaozeng/">LinkedIn</a> | 
                <a href="CV-Ziyao Zeng.pdf">CV</a> | 
              </li>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/headshot.JPG"><img style="width:100%;max-width:100%" alt="profile photo" src="images/headshot.JPG" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr><td style="padding:20px;width:100%;vertical-align:middle">
        <font color="red" size="4"><heading><b>How about my education</b> </heading></font>
          <li>[Sept. 2019 - (Expected) June 2023] <em>B.Eng.</em> in Computer Science, School of Imformation Science and Technology, ShanghaiTech University</li>
          <li>[Sept. 2019 - (Expected) June 2023] <em>Minor.</em> in Innovation and Entrepreneurship, School of Entrepreneurship and Management, ShanghaiTech University</li>
          <li>[Sept. 2016 - June 2019] Fuzhou NO.1 High School</li>  




        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr><td style="padding:20px;width:100%;vertical-align:middle">

            <font color="red" size="4"><heading><b>How about my research?</b></heading></font>
            <p>My research interest includes Computer Vision, Bioinformatics, Machine Learning and Deep Learning.</p>
            <p><img style="width:30%;max-width:100%" src ="images/plus.png" align ="left">I'm currently a research assistant of <a href="http://plus.sist.shanghaitech.edu.cn/">PLUS Lab</a> of <a href="https://www.shanghaitech.edu.cn/">ShanghaiTech University</a> supervised by <a href="https://xmhe.bitbucket.io/">Prof. Xuming He</a>
            <p><img style="width:30%;max-width:100%" src ="images/grasp.png" align ="left"> Besides, I'm also a research assistant of <a href="https://www.grasp.upenn.edu/">GRASP Lab</a> of <a href="https://www.upenn.edu/">University of Pennsylvania</a> supervised by <a href="https://www.cis.upenn.edu/~jshi/">Prof. Jianbo Shi</a>
            
        </td></tr></tbody></table>
        <!-- </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
	      <img style="width:100%;max-width:100%" alt="shape completion" src="images/headshot.JPG">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Estimating Geometry of Manipulated Objects via Unexpected Environmental Contact</papertitle>
              <br>
              <strong>Ziyao Zeng</strong>,
	      <a href="https://robertomartinmartin.com/">Roberto Martin-Martin</a>,
              <a href="http://stanford.edu/~mishlee/">Michelle Lee</a>,
              <a href="https://profiles.stanford.edu/silvio-savarese">Silvio Savarese</a>,
              <a href="https://web.stanford.edu/~bohg/">Jeannette Bohg</a>,
              <br>
              <em>in progress</em>
              <br>
              <p></p>
              <p>Many tasks require geometric reasoning best done with a 3D model of the object to be manipulated, which can be estimated during the execution of a task using vision and touch.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
	      <img style="width:100%;max-width:100%" alt="wiping task" src="images/vices.jpg">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://stanfordvl.github.io/vices/">
                <papertitle>Variable Impedance Control in End-Effector Space: An Action Space for Reinforcement Learning in Contact-Rich Tasks</papertitle>
              </a>
              <br>
              
              <a href="https://robertomartinmartin.com/">Roberto Martin-Martin</a>,
              <a href="http://stanford.edu/~mishlee/">Michelle Lee</a>,
              <strong>Ziyao Zeng</strong>,
              <a href="https://profiles.stanford.edu/silvio-savarese">Silvio Savarese</a>,
              <a href="https://web.stanford.edu/~bohg/">Jeannette Bohg</a>,
              <a href="https://animesh.garg.tech/">Animesh Garg</a>
              <br>
              <em>IROS</em>, 2019  
              <br>
              <a href="https://stanfordvl.github.io/vices/">project page</a> /
              <a href="https://arxiv.org/abs/1906.08880">arXiv</a> /
	      <a href="https://github.com/StanfordVL/robosuite/tree/vices_iros19">code</a> /
              <a href="https://www.youtube.com/watch?v=AozIUIW3Ghs">video</a>
              <p></p>
              <p>When doing reinforcement learning, using actions in end-effector space and allowing the agent to control its own impedance parameters results in better generalizability across robots and better sample efficiency.
              </p>
            </td>
          </tr>
	  
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
	      <img style="width:100%;max-width:100%" alt="BERT model diagram" src="images/headshot.JPG">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2011.04883">
                <papertitle>Determining Question-Answer Plausibility in Crowdsourced Datasets Using Multi-Task Learning</papertitle>
              </a>
              <br>
              <strong>Ziyao Zeng</strong>,
	      <a href="https://maya-varma.com/">Maya Varma</a>,
	      <a href="https://www.linkedin.com/in/clarezhu/">Clare Zhu</a>,
              <a href="http://www.ranjaykrishna.com/">Ranjay Krishna</a>
              <br>
              <em>EMNLP Workshop on Noisy User-Generated Text</em>, 2020  
              <br>
              <a href="https://arxiv.org/abs/2011.04883">arXiv</a> /
	      <a href="https://github.com/rachel-1/qa_plausibility">code</a>
              <p></p>
              <p>Given (possibly bot-generated) questions and natural language user responses, we can estimate the plausibility of the question and answer pair in order to filter the data to create large-scale datasets and enable the creation of active learning agents.
              </p>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
	      <img style="width:100%;max-width:100%" alt="radiographs with highlighted abnormalities" src="images/headshot.JPG">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www-nature-com.stanford.idm.oclc.org/articles/s42256-019-0126-0">
                <papertitle>Automated abnormality detection in lower extremity radiographs using deep learning</papertitle>
              </a>
              <br>
	      <a href="https://maya-varma.com/">Maya Varma</a>,
	      <a href="https://www.linkedin.com/in/mandy-lu-9b41a861/">Mandy Lu</a>*, <strong>Ziyao Zeng*</strong>,
	      <a href="https://jdunnmon.github.io/">Jared Dunnmon</a>,
	      <a href="https://www.linkedin.com/in/nishith-khandwala-16b27227/">Nishith Khandwala</a>,
	      <a href="https://rajpurkar.github.io/">Pranav Rajpurkar</a>,
	      Jin Long,
	      <a href="https://profiles.stanford.edu/christopher-beaulieu">Christopher Beaulieu</a>,
	      <a href="https://www.linkedin.com/in/katie-shpanskaya-88823935/">Katie Shpanskaya</a>,
	      <a href="https://profiles.stanford.edu/fei-fei-li">Fei-Fei Li</a>,
	      <a href="https://www.linkedin.com/in/mattlungrenmd/">Matthew P. Lungren</a>*,
	      <a href="https://www.linkedin.com/in/bhavikpatelmdmba/">Bhavik N. Patel</a>*
              <br>
              <em>Nature Machine Intelligence</em>, 2019  
              <br>
              <a href="https://www-nature-com.stanford.idm.oclc.org/articles/s42256-019-0126-0">Nature</a> /
	      <a href="https://github.com/maya124/MSK-LE">code</a> /
	      <a href="https://aimi.stanford.edu/lera-lower-extremity-radiographs-2">data</a>
              <p></p>
              <p>A single CNN model can be achieve high accuracy for abnormality classification in radiographs even across body parts.
              </p>
            </td>
          </tr> 
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Teaching</heading>
              <p>
		<ul>
		  <li><a href="http://web.stanford.edu/class/cs224n/">CS224N: Natural Language Processing with Deep Learning</a> [1 quarter]</li>
		  <li><a href="http://web.stanford.edu/class/cs106x/">CS106X: Programming Abstractions (Accelerated)</a> [2 quarters]</li>
		  <li><a href="http://web.stanford.edu/class/cs106b/">CS106B: Programming Abstractions</a> [2 quarters]</li>
		  <li><a href="http://web.stanford.edu/class/cs106a/">CS106A: Programming Methodologies</a> [1 quarter]</li>
		</ul>
              </p>
            </td>
          </tr>
        </tbody></table> -->
	
	<p style="text-align:right">Website format from <a href="https://rachel-gardner.com/">Rachel Gardner</a>.</p>
</body>

</html>
